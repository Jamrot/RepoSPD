{"idx": 0, "category": "non-security", "commit_message": " http: cosmetics: reformat reconnect check for better readability The condition was a bit too long, and most editors will break the line&&&&and turn it into an unreadable mess. Move out some of the conditions.&&&&&&&&This should not change the behavior.&&&& ", "diff_code": "diff --git a/libavformat/http.c b/libavformat/http.c\nindex 510b233..aa80e31 100644\n--- a/libavformat/http.c\n+++ b/libavformat/http.c\n@@ -1449,12 +1449,18 @@ static int http_read_stream(URLContext *h, uint8_t *buf, int size)\n         return http_buf_read_compressed(h, buf, size);\n #endif /* CONFIG_ZLIB */\n     read_ret = http_buf_read(h, buf, size);\n-    while ((read_ret  < 0           && s->reconnect        && (!h->is_streamed || s->reconnect_streamed) && s->filesize > 0 && s->off < s->filesize)\n-        || (read_ret == AVERROR_EOF && s->reconnect_at_eof && (!h->is_streamed || s->reconnect_streamed))) {\n+    while (read_ret < 0) {\n         uint64_t target = h->is_streamed ? 0 : s->off;\n \n         if (read_ret == AVERROR_EXIT)\n-            return read_ret;\n+            break;\n+\n+        if (h->is_streamed && !s->reconnect_streamed)\n+            break;\n+\n+        if (!(s->reconnect && s->filesize > 0 && s->off < s->filesize) &&\n+            !(s->reconnect_at_eof && read_ret == AVERROR_EOF))\n+            break;\n \n         if (reconnect_delay > s->reconnect_delay_max)\n             return AVERROR(EIO);\n", "ori_dataset": "ffmpeg", "commit_id": "43e1ccfea1", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 1, "category": "non-security", "commit_message": " avcodec/nvenc: add some more error case checks Signed-off-by: Timo Rothenpieler <timo@rothenpieler.org>&&&& ", "diff_code": "diff --git a/libavcodec/nvenc.c b/libavcodec/nvenc.c\nindex a819423..7038a49 100644\n--- a/libavcodec/nvenc.c\n+++ b/libavcodec/nvenc.c\n@@ -1529,6 +1529,7 @@ static int nvenc_find_free_reg_resource(AVCodecContext *avctx)\n     NvencContext *ctx = avctx->priv_data;\n     NvencDynLoadFunctions *dl_fn = &ctx->nvenc_dload_funcs;\n     NV_ENCODE_API_FUNCTION_LIST *p_nvenc = &dl_fn->nvenc_funcs;\n+    NVENCSTATUS nv_status;\n \n     int i;\n \n@@ -1536,8 +1537,9 @@ static int nvenc_find_free_reg_resource(AVCodecContext *avctx)\n         for (i = 0; i < ctx->nb_registered_frames; i++) {\n             if (!ctx->registered_frames[i].mapped) {\n                 if (ctx->registered_frames[i].regptr) {\n-                    p_nvenc->nvEncUnregisterResource(ctx->nvencoder,\n-                                                ctx->registered_frames[i].regptr);\n+                    nv_status = p_nvenc->nvEncUnregisterResource(ctx->nvencoder, ctx->registered_frames[i].regptr);\n+                    if (nv_status != NV_ENC_SUCCESS)\n+                        return nvenc_print_error(avctx, nv_status, \"Failed unregistering unused input resource\");\n                     ctx->registered_frames[i].regptr = NULL;\n                 }\n                 return i;\n@@ -1789,15 +1791,25 @@ static int process_output_surface(AVCodecContext *avctx, AVPacket *pkt, NvencSur\n     memcpy(pkt->data, lock_params.bitstreamBufferPtr, lock_params.bitstreamSizeInBytes);\n \n     nv_status = p_nvenc->nvEncUnlockBitstream(ctx->nvencoder, tmpoutsurf->output_surface);\n-    if (nv_status != NV_ENC_SUCCESS)\n-        nvenc_print_error(avctx, nv_status, \"Failed unlocking bitstream buffer, expect the gates of mordor to open\");\n+    if (nv_status != NV_ENC_SUCCESS) {\n+        res = nvenc_print_error(avctx, nv_status, \"Failed unlocking bitstream buffer, expect the gates of mordor to open\");\n+        goto error;\n+    }\n \n \n     if (avctx->pix_fmt == AV_PIX_FMT_CUDA || avctx->pix_fmt == AV_PIX_FMT_D3D11) {\n         ctx->registered_frames[tmpoutsurf->reg_idx].mapped -= 1;\n         if (ctx->registered_frames[tmpoutsurf->reg_idx].mapped == 0) {\n-            p_nvenc->nvEncUnmapInputResource(ctx->nvencoder, ctx->registered_frames[tmpoutsurf->reg_idx].in_map.mappedResource);\n-            p_nvenc->nvEncUnregisterResource(ctx->nvencoder, ctx->registered_frames[tmpoutsurf->reg_idx].regptr);\n+            nv_status = p_nvenc->nvEncUnmapInputResource(ctx->nvencoder, ctx->registered_frames[tmpoutsurf->reg_idx].in_map.mappedResource);\n+            if (nv_status != NV_ENC_SUCCESS) {\n+                res = nvenc_print_error(avctx, nv_status, \"Failed unmapping input resource\");\n+                goto error;\n+            }\n+            nv_status = p_nvenc->nvEncUnregisterResource(ctx->nvencoder, ctx->registered_frames[tmpoutsurf->reg_idx].regptr);\n+            if (nv_status != NV_ENC_SUCCESS) {\n+                res = nvenc_print_error(avctx, nv_status, \"Failed unregistering input resource\");\n+                goto error;\n+            }\n             ctx->registered_frames[tmpoutsurf->reg_idx].regptr = NULL;\n         } else if (ctx->registered_frames[tmpoutsurf->reg_idx].mapped < 0) {\n             res = AVERROR_BUG;\n", "ori_dataset": "ffmpeg", "commit_id": "48e52e4edd", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 3, "category": "non-security", "commit_message": " x86inc: Enable AVX emulation for floating-point pseudo-instructions There are 32 pseudo-instructions for each floating-point comparison&&&&instruction, but only 8 of them are actually valid in legacy-encoded mode.&&&&The remaining 24 requires the use of VEX-encoded (v-prefixed) instructions&&&&and can therefore be disregarded for this purpose.&&&& ", "diff_code": "diff --git a/libavutil/x86/x86inc.asm b/libavutil/x86/x86inc.asm\nindex 196374c..3b43dbc 100644\n--- a/libavutil/x86/x86inc.asm\n+++ b/libavutil/x86/x86inc.asm\n@@ -1297,10 +1297,42 @@ AVX_INSTR blendpd, sse4, 1, 1, 0\n AVX_INSTR blendps, sse4, 1, 1, 0\n AVX_INSTR blendvpd, sse4 ; can't be emulated\n AVX_INSTR blendvps, sse4 ; can't be emulated\n+AVX_INSTR cmpeqpd, sse2, 1, 0, 1\n+AVX_INSTR cmpeqps, sse, 1, 0, 1\n+AVX_INSTR cmpeqsd, sse2, 1, 0, 0\n+AVX_INSTR cmpeqss, sse, 1, 0, 0\n+AVX_INSTR cmplepd, sse2, 1, 0, 0\n+AVX_INSTR cmpleps, sse, 1, 0, 0\n+AVX_INSTR cmplesd, sse2, 1, 0, 0\n+AVX_INSTR cmpless, sse, 1, 0, 0\n+AVX_INSTR cmpltpd, sse2, 1, 0, 0\n+AVX_INSTR cmpltps, sse, 1, 0, 0\n+AVX_INSTR cmpltsd, sse2, 1, 0, 0\n+AVX_INSTR cmpltss, sse, 1, 0, 0\n+AVX_INSTR cmpneqpd, sse2, 1, 0, 1\n+AVX_INSTR cmpneqps, sse, 1, 0, 1\n+AVX_INSTR cmpneqsd, sse2, 1, 0, 0\n+AVX_INSTR cmpneqss, sse, 1, 0, 0\n+AVX_INSTR cmpnlepd, sse2, 1, 0, 0\n+AVX_INSTR cmpnleps, sse, 1, 0, 0\n+AVX_INSTR cmpnlesd, sse2, 1, 0, 0\n+AVX_INSTR cmpnless, sse, 1, 0, 0\n+AVX_INSTR cmpnltpd, sse2, 1, 0, 0\n+AVX_INSTR cmpnltps, sse, 1, 0, 0\n+AVX_INSTR cmpnltsd, sse2, 1, 0, 0\n+AVX_INSTR cmpnltss, sse, 1, 0, 0\n+AVX_INSTR cmpordpd, sse2 1, 0, 1\n+AVX_INSTR cmpordps, sse 1, 0, 1\n+AVX_INSTR cmpordsd, sse2 1, 0, 0\n+AVX_INSTR cmpordss, sse 1, 0, 0\n AVX_INSTR cmppd, sse2, 1, 1, 0\n AVX_INSTR cmpps, sse, 1, 1, 0\n AVX_INSTR cmpsd, sse2, 1, 1, 0\n AVX_INSTR cmpss, sse, 1, 1, 0\n+AVX_INSTR cmpunordpd, sse2, 1, 0, 1\n+AVX_INSTR cmpunordps, sse, 1, 0, 1\n+AVX_INSTR cmpunordsd, sse2, 1, 0, 0\n+AVX_INSTR cmpunordss, sse, 1, 0, 0\n AVX_INSTR comisd, sse2\n AVX_INSTR comiss, sse\n AVX_INSTR cvtdq2pd, sse2\n", "ori_dataset": "ffmpeg", "commit_id": "3a02cbe3fa", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 4, "category": "non-security", "commit_message": " avformat/hlsenc: Check that data is set If codecpar->extradata is not set (for example, when the stream goes&&&&through the 'tee' muxer), then a segfault occurs.&&&&This patch ensures the data variable is not null before attempting&&&&to access it&&&&Before the var_stream_map option was available - I was using the tee&&&&muxer to create each resolution as an individual stream.&&&&When running this configuration after the most recent hlsenc change&&&&I hit a segfault&&&&The most simple command which recreates the segfault is:&&&&ffmpeg -i in.ts -map 0:a -map 0:v -c:a aac -c:v h264 -f tee [select=\\'a,v\\':f=hls]tv_hls_hd.m3u8&&&&&&&&Signed-off-by: Brendan McGrath <redmcg@redmandi.dyndns.org>&&&& ", "diff_code": "diff --git a/libavformat/hlsenc.c b/libavformat/hlsenc.c\nindex 8ad906a..42e437f 100644\n--- a/libavformat/hlsenc.c\n+++ b/libavformat/hlsenc.c\n@@ -308,7 +308,7 @@ static void write_codec_attr(AVStream *st, VariantStream *vs) {\n \n     if (st->codecpar->codec_id == AV_CODEC_ID_H264) {\n         uint8_t *data = st->codecpar->extradata;\n-        if ((data[0] | data[1] | data[2]) == 0 && data[3] == 1 && (data[4] & 0x1F) == 7) {\n+        if (data && (data[0] | data[1] | data[2]) == 0 && data[3] == 1 && (data[4] & 0x1F) == 7) {\n             snprintf(attr, sizeof(attr),\n                      \"avc1.%02x%02x%02x\", data[5], data[6], data[7]);\n         } else {\n", "ori_dataset": "ffmpeg", "commit_id": "2472dbc7a7", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 6, "category": "non-security", "commit_message": " ffmpeg: use explicitly requested hwaccel only With there being two hwaccels that use the CUDA pix_fmt now, just&&&&relying on the pix_fmt to identify the selected hwaccel is not enough&&&&anymore.&&&&&&&&So this checks if the user explicitly selected a hwaccel, and only&&&&accepts that one.&&&& ", "diff_code": "diff --git a/fftools/ffmpeg.c b/fftools/ffmpeg.c\nindex 0aff1d3..db391f7 100644\n--- a/fftools/ffmpeg.c\n+++ b/fftools/ffmpeg.c\n@@ -2782,11 +2782,12 @@ fail:\n     av_freep(&avc);\n }\n \n-static const HWAccel *get_hwaccel(enum AVPixelFormat pix_fmt)\n+static const HWAccel *get_hwaccel(enum AVPixelFormat pix_fmt, enum HWAccelID selected_hwaccel_id)\n {\n     int i;\n     for (i = 0; hwaccels[i].name; i++)\n-        if (hwaccels[i].pix_fmt == pix_fmt)\n+        if (hwaccels[i].pix_fmt == pix_fmt &&\n+            (!selected_hwaccel_id || selected_hwaccel_id == HWACCEL_AUTO || hwaccels[i].id == selected_hwaccel_id))\n             return &hwaccels[i];\n     return NULL;\n }\n@@ -2804,7 +2805,7 @@ static enum AVPixelFormat get_format(AVCodecContext *s, const enum AVPixelFormat\n         if (!(desc->flags & AV_PIX_FMT_FLAG_HWACCEL))\n             break;\n \n-        hwaccel = get_hwaccel(*p);\n+        hwaccel = get_hwaccel(*p, ist->hwaccel_id);\n         if (!hwaccel ||\n             (ist->active_hwaccel_id && ist->active_hwaccel_id != hwaccel->id) ||\n             (ist->hwaccel_id != HWACCEL_AUTO && ist->hwaccel_id != hwaccel->id))\n", "ori_dataset": "ffmpeg", "commit_id": "6a4d1c9063", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 8, "category": "non-security", "commit_message": " avcodec/dca_core: always limit frame size to data size Silences pointless error message when decoding DTS-in-WAV stream with&&&&excessive frame size stored in header.&&&& ", "diff_code": "diff --git a/libavcodec/dca_core.c b/libavcodec/dca_core.c\nindex 6cb1f30..accc5ef 100644\n--- a/libavcodec/dca_core.c\n+++ b/libavcodec/dca_core.c\n@@ -1816,7 +1816,7 @@ int ff_dca_core_parse(DCACoreDecoder *s, uint8_t *data, int size)\n         return ret;\n \n     // Workaround for DTS in WAV\n-    if (s->frame_size > size && s->frame_size < size + 4)\n+    if (s->frame_size > size)\n         s->frame_size = size;\n \n     if (ff_dca_seek_bits(&s->gb, s->frame_size * 8)) {\n", "ori_dataset": "ffmpeg", "commit_id": "73789b85a7", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 9, "category": "non-security", "commit_message": " Don't use _tzcnt instrinics with clang for windows w/o BMI. Technically _tzcnt* intrinsics are only available when the BMI&&&&instruction set is present. However the instruction encoding&&&&degrades to \"rep bsf\" on older processors.&&&&&&&&Clang for Windows debatably restricts the _tzcnt* instrinics behind&&&&the __BMI__ architecture define, so check for its presence or&&&&exclude the usage of these intrinics when clang is present.&&&&&&&&See also:&&&&https://ffmpeg.org/pipermail/ffmpeg-devel/2015-November/183404.html&&&&https://bugs.llvm.org/show_bug.cgi?id=30506&&&&http://lists.llvm.org/pipermail/cfe-dev/2016-October/051034.html&&&&&&&&Signed-off-by: Dale Curtis <dalecurtis@chromium.org>&&&&Reviewed-by: Matt Oliver <protogonoi@gmail.com>&&&&Signed-off-by: Michael Niedermayer <michael@niedermayer.cc>&&&& ", "diff_code": "diff --git a/libavutil/x86/intmath.h b/libavutil/x86/intmath.h\nindex e83971c..40743fd 100644\n--- a/libavutil/x86/intmath.h\n+++ b/libavutil/x86/intmath.h\n@@ -47,7 +47,8 @@ static av_always_inline av_const int ff_log2_x86(unsigned int v)\n #   endif\n #   define ff_log2_16bit av_log2\n \n-#if defined(__INTEL_COMPILER) || (defined(_MSC_VER) && (_MSC_VER >= 1700))\n+#if defined(__INTEL_COMPILER) || (defined(_MSC_VER) && (_MSC_VER >= 1700) && \\\n+                                  (defined(__BMI__) || !defined(__clang__)))\n #   define ff_ctz(v) _tzcnt_u32(v)\n \n #   if ARCH_X86_64\n", "ori_dataset": "ffmpeg", "commit_id": "50e30d9bb7", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 10, "category": "non-security", "commit_message": " avcodec/mips: Unrolled loops and expanded functions in avc put mc 10 & 30 msa functions Signed-off-by: Kaustubh Raste <kaustubh.raste@imgtec.com>&&&&Reviewed-by: Manojkumar Bhosale <Manojkumar.Bhosale@imgtec.com>&&&&Signed-off-by: Michael Niedermayer <michael@niedermayer.cc>&&&& ", "diff_code": "diff --git a/libavcodec/mips/h264qpel_msa.c b/libavcodec/mips/h264qpel_msa.c\nindex 05dffea..b7f6c3d 100644\n--- a/libavcodec/mips/h264qpel_msa.c\n+++ b/libavcodec/mips/h264qpel_msa.c\n@@ -3065,37 +3065,309 @@ void ff_avg_h264_qpel4_mc00_msa(uint8_t *dst, const uint8_t *src,\n void ff_put_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,\n                                  ptrdiff_t stride)\n {\n-    avc_luma_hz_qrt_16w_msa(src - 2, stride, dst, stride, 16, 0);\n+    uint32_t loop_cnt;\n+    v16i8 dst0, dst1, dst2, dst3, src0, src1, src2, src3, src4, src5, src6;\n+    v16i8 mask0, mask1, mask2, mask3, mask4, mask5, src7, vec11;\n+    v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9, vec10;\n+    v8i16 res0, res1, res2, res3, res4, res5, res6, res7;\n+    v16i8 minus5b = __msa_ldi_b(-5);\n+    v16i8 plus20b = __msa_ldi_b(20);\n+\n+    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n+    mask3 = mask0 + 8;\n+    mask4 = mask1 + 8;\n+    mask5 = mask2 + 8;\n+    src -= 2;\n+\n+    for (loop_cnt = 4; loop_cnt--;) {\n+        LD_SB2(src, 16, src0, src1);\n+        src += stride;\n+        LD_SB2(src, 16, src2, src3);\n+        src += stride;\n+        LD_SB2(src, 16, src4, src5);\n+        src += stride;\n+        LD_SB2(src, 16, src6, src7);\n+        src += stride;\n+\n+        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);\n+        VSHF_B2_SB(src0, src0, src0, src1, mask0, mask3, vec0, vec3);\n+        VSHF_B2_SB(src2, src2, src2, src3, mask0, mask3, vec6, vec9);\n+        VSHF_B2_SB(src0, src0, src0, src1, mask1, mask4, vec1, vec4);\n+        VSHF_B2_SB(src2, src2, src2, src3, mask1, mask4, vec7, vec10);\n+        VSHF_B2_SB(src0, src0, src0, src1, mask2, mask5, vec2, vec5);\n+        VSHF_B2_SB(src2, src2, src2, src3, mask2, mask5, vec8, vec11);\n+        HADD_SB4_SH(vec0, vec3, vec6, vec9, res0, res1, res2, res3);\n+        DPADD_SB4_SH(vec1, vec4, vec7, vec10, minus5b, minus5b, minus5b,\n+                     minus5b, res0, res1, res2, res3);\n+        DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,\n+                     plus20b, res0, res1, res2, res3);\n+        VSHF_B2_SB(src4, src4, src4, src5, mask0, mask3, vec0, vec3);\n+        VSHF_B2_SB(src6, src6, src6, src7, mask0, mask3, vec6, vec9);\n+        VSHF_B2_SB(src4, src4, src4, src5, mask1, mask4, vec1, vec4);\n+        VSHF_B2_SB(src6, src6, src6, src7, mask1, mask4, vec7, vec10);\n+        VSHF_B2_SB(src4, src4, src4, src5, mask2, mask5, vec2, vec5);\n+        VSHF_B2_SB(src6, src6, src6, src7, mask2, mask5, vec8, vec11);\n+        HADD_SB4_SH(vec0, vec3, vec6, vec9, res4, res5, res6, res7);\n+        DPADD_SB4_SH(vec1, vec4, vec7, vec10, minus5b, minus5b, minus5b,\n+                     minus5b, res4, res5, res6, res7);\n+        DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,\n+                     plus20b, res4, res5, res6, res7);\n+        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 2);\n+        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 2);\n+        SRARI_H4_SH(res0, res1, res2, res3, 5);\n+        SRARI_H4_SH(res4, res5, res6, res7, 5);\n+        SAT_SH4_SH(res0, res1, res2, res3, 7);\n+        SAT_SH4_SH(res4, res5, res6, res7, 7);\n+        PCKEV_B2_SB(res1, res0, res3, res2, dst0, dst1);\n+        PCKEV_B2_SB(res5, res4, res7, res6, dst2, dst3);\n+        dst0 = __msa_aver_s_b(dst0, src0);\n+        dst1 = __msa_aver_s_b(dst1, src2);\n+        dst2 = __msa_aver_s_b(dst2, src4);\n+        dst3 = __msa_aver_s_b(dst3, src6);\n+        XORI_B4_128_SB(dst0, dst1, dst2, dst3);\n+        ST_SB4(dst0, dst1, dst2, dst3, dst, stride);\n+        dst += (4 * stride);\n+    }\n }\n \n void ff_put_h264_qpel16_mc30_msa(uint8_t *dst, const uint8_t *src,\n                                  ptrdiff_t stride)\n {\n-    avc_luma_hz_qrt_16w_msa(src - 2, stride, dst, stride, 16, 1);\n+    uint32_t loop_cnt;\n+    v16i8 dst0, dst1, dst2, dst3, src0, src1, src2, src3, src4, src5, src6;\n+    v16i8 mask0, mask1, mask2, mask3, mask4, mask5, src7, vec11;\n+    v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9, vec10;\n+    v8i16 res0, res1, res2, res3, res4, res5, res6, res7;\n+    v16i8 minus5b = __msa_ldi_b(-5);\n+    v16i8 plus20b = __msa_ldi_b(20);\n+\n+    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n+    mask3 = mask0 + 8;\n+    mask4 = mask1 + 8;\n+    mask5 = mask2 + 8;\n+    src -= 2;\n+\n+    for (loop_cnt = 4; loop_cnt--;) {\n+        LD_SB2(src, 16, src0, src1);\n+        src += stride;\n+        LD_SB2(src, 16, src2, src3);\n+        src += stride;\n+        LD_SB2(src, 16, src4, src5);\n+        src += stride;\n+        LD_SB2(src, 16, src6, src7);\n+        src += stride;\n+\n+        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);\n+        VSHF_B2_SB(src0, src0, src0, src1, mask0, mask3, vec0, vec3);\n+        VSHF_B2_SB(src2, src2, src2, src3, mask0, mask3, vec6, vec9);\n+        VSHF_B2_SB(src0, src0, src0, src1, mask1, mask4, vec1, vec4);\n+        VSHF_B2_SB(src2, src2, src2, src3, mask1, mask4, vec7, vec10);\n+        VSHF_B2_SB(src0, src0, src0, src1, mask2, mask5, vec2, vec5);\n+        VSHF_B2_SB(src2, src2, src2, src3, mask2, mask5, vec8, vec11);\n+        HADD_SB4_SH(vec0, vec3, vec6, vec9, res0, res1, res2, res3);\n+        DPADD_SB4_SH(vec1, vec4, vec7, vec10, minus5b, minus5b, minus5b,\n+                     minus5b, res0, res1, res2, res3);\n+        DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,\n+                     plus20b, res0, res1, res2, res3);\n+        VSHF_B2_SB(src4, src4, src4, src5, mask0, mask3, vec0, vec3);\n+        VSHF_B2_SB(src6, src6, src6, src7, mask0, mask3, vec6, vec9);\n+        VSHF_B2_SB(src4, src4, src4, src5, mask1, mask4, vec1, vec4);\n+        VSHF_B2_SB(src6, src6, src6, src7, mask1, mask4, vec7, vec10);\n+        VSHF_B2_SB(src4, src4, src4, src5, mask2, mask5, vec2, vec5);\n+        VSHF_B2_SB(src6, src6, src6, src7, mask2, mask5, vec8, vec11);\n+        HADD_SB4_SH(vec0, vec3, vec6, vec9, res4, res5, res6, res7);\n+        DPADD_SB4_SH(vec1, vec4, vec7, vec10, minus5b, minus5b, minus5b,\n+                     minus5b, res4, res5, res6, res7);\n+        DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,\n+                     plus20b, res4, res5, res6, res7);\n+        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 3);\n+        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 3);\n+        SRARI_H4_SH(res0, res1, res2, res3, 5);\n+        SRARI_H4_SH(res4, res5, res6, res7, 5);\n+        SAT_SH4_SH(res0, res1, res2, res3, 7);\n+        SAT_SH4_SH(res4, res5, res6, res7, 7);\n+        PCKEV_B2_SB(res1, res0, res3, res2, dst0, dst1);\n+        PCKEV_B2_SB(res5, res4, res7, res6, dst2, dst3);\n+        dst0 = __msa_aver_s_b(dst0, src0);\n+        dst1 = __msa_aver_s_b(dst1, src2);\n+        dst2 = __msa_aver_s_b(dst2, src4);\n+        dst3 = __msa_aver_s_b(dst3, src6);\n+        XORI_B4_128_SB(dst0, dst1, dst2, dst3);\n+        ST_SB4(dst0, dst1, dst2, dst3, dst, stride);\n+        dst += (4 * stride);\n+    }\n }\n \n void ff_put_h264_qpel8_mc10_msa(uint8_t *dst, const uint8_t *src,\n                                 ptrdiff_t stride)\n {\n-    avc_luma_hz_qrt_8w_msa(src - 2, stride, dst, stride, 8, 0);\n+    v16i8 src0, src1, src2, src3, src4, src5, src6, src7, mask0, mask1, mask2;\n+    v16i8 tmp0, tmp1, tmp2, tmp3, vec11;\n+    v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9, vec10;\n+    v8i16 res0, res1, res2, res3, res4, res5, res6, res7;\n+    v16i8 minus5b = __msa_ldi_b(-5);\n+    v16i8 plus20b = __msa_ldi_b(20);\n+\n+    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n+    LD_SB8(src - 2, stride, src0, src1, src2, src3, src4, src5, src6, src7);\n+    XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);\n+    VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec0, vec1);\n+    VSHF_B2_SB(src2, src2, src3, src3, mask0, mask0, vec2, vec3);\n+    HADD_SB4_SH(vec0, vec1, vec2, vec3, res0, res1, res2, res3);\n+    VSHF_B2_SB(src0, src0, src1, src1, mask1, mask1, vec4, vec5);\n+    VSHF_B2_SB(src2, src2, src3, src3, mask1, mask1, vec6, vec7);\n+    DPADD_SB4_SH(vec4, vec5, vec6, vec7, minus5b, minus5b, minus5b, minus5b,\n+                 res0, res1, res2, res3);\n+    VSHF_B2_SB(src0, src0, src1, src1, mask2, mask2, vec8, vec9);\n+    VSHF_B2_SB(src2, src2, src3, src3, mask2, mask2, vec10, vec11);\n+    DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,\n+                 res0, res1, res2, res3);\n+    VSHF_B2_SB(src4, src4, src5, src5, mask0, mask0, vec0, vec1);\n+    VSHF_B2_SB(src6, src6, src7, src7, mask0, mask0, vec2, vec3);\n+    HADD_SB4_SH(vec0, vec1, vec2, vec3, res4, res5, res6, res7);\n+    VSHF_B2_SB(src4, src4, src5, src5, mask1, mask1, vec4, vec5);\n+    VSHF_B2_SB(src6, src6, src7, src7, mask1, mask1, vec6, vec7);\n+    DPADD_SB4_SH(vec4, vec5, vec6, vec7, minus5b, minus5b, minus5b, minus5b,\n+                 res4, res5, res6, res7);\n+    VSHF_B2_SB(src4, src4, src5, src5, mask2, mask2, vec8, vec9);\n+    VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);\n+    DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,\n+                 res4, res5, res6, res7);\n+    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);\n+    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);\n+    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 2);\n+    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 2);\n+    PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);\n+    PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);\n+    SRARI_H4_SH(res0, res1, res2, res3, 5);\n+    SRARI_H4_SH(res4, res5, res6, res7, 5);\n+    SAT_SH4_SH(res0, res1, res2, res3, 7);\n+    SAT_SH4_SH(res4, res5, res6, res7, 7);\n+    PCKEV_B2_SB(res1, res0, res3, res2, tmp0, tmp1);\n+    PCKEV_B2_SB(res5, res4, res7, res6, tmp2, tmp3);\n+    tmp0 = __msa_aver_s_b(tmp0, src0);\n+    tmp1 = __msa_aver_s_b(tmp1, src1);\n+    tmp2 = __msa_aver_s_b(tmp2, src4);\n+    tmp3 = __msa_aver_s_b(tmp3, src5);\n+    XORI_B4_128_SB(tmp0, tmp1, tmp2, tmp3);\n+    ST8x8_UB(tmp0, tmp1, tmp2, tmp3, dst, stride);\n }\n \n void ff_put_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,\n                                 ptrdiff_t stride)\n {\n-    avc_luma_hz_qrt_8w_msa(src - 2, stride, dst, stride, 8, 1);\n+    v16i8 src0, src1, src2, src3, src4, src5, src6, src7, mask0, mask1, mask2;\n+    v16i8 tmp0, tmp1, tmp2, tmp3, vec11;\n+    v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9, vec10;\n+    v8i16 res0, res1, res2, res3, res4, res5, res6, res7;\n+    v16i8 minus5b = __msa_ldi_b(-5);\n+    v16i8 plus20b = __msa_ldi_b(20);\n+\n+    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n+    LD_SB8(src - 2, stride, src0, src1, src2, src3, src4, src5, src6, src7);\n+    XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);\n+    VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec0, vec1);\n+    VSHF_B2_SB(src2, src2, src3, src3, mask0, mask0, vec2, vec3);\n+    HADD_SB4_SH(vec0, vec1, vec2, vec3, res0, res1, res2, res3);\n+    VSHF_B2_SB(src0, src0, src1, src1, mask1, mask1, vec4, vec5);\n+    VSHF_B2_SB(src2, src2, src3, src3, mask1, mask1, vec6, vec7);\n+    DPADD_SB4_SH(vec4, vec5, vec6, vec7, minus5b, minus5b, minus5b, minus5b,\n+                 res0, res1, res2, res3);\n+    VSHF_B2_SB(src0, src0, src1, src1, mask2, mask2, vec8, vec9);\n+    VSHF_B2_SB(src2, src2, src3, src3, mask2, mask2, vec10, vec11);\n+    DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,\n+                 res0, res1, res2, res3);\n+    VSHF_B2_SB(src4, src4, src5, src5, mask0, mask0, vec0, vec1);\n+    VSHF_B2_SB(src6, src6, src7, src7, mask0, mask0, vec2, vec3);\n+    HADD_SB4_SH(vec0, vec1, vec2, vec3, res4, res5, res6, res7);\n+    VSHF_B2_SB(src4, src4, src5, src5, mask1, mask1, vec4, vec5);\n+    VSHF_B2_SB(src6, src6, src7, src7, mask1, mask1, vec6, vec7);\n+    DPADD_SB4_SH(vec4, vec5, vec6, vec7, minus5b, minus5b, minus5b, minus5b,\n+                 res4, res5, res6, res7);\n+    VSHF_B2_SB(src4, src4, src5, src5, mask2, mask2, vec8, vec9);\n+    VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);\n+    DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,\n+                 res4, res5, res6, res7);\n+    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);\n+    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);\n+    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 3);\n+    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 3);\n+    PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);\n+    PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);\n+    SRARI_H4_SH(res0, res1, res2, res3, 5);\n+    SRARI_H4_SH(res4, res5, res6, res7, 5);\n+    SAT_SH4_SH(res0, res1, res2, res3, 7);\n+    SAT_SH4_SH(res4, res5, res6, res7, 7);\n+    PCKEV_B2_SB(res1, res0, res3, res2, tmp0, tmp1);\n+    PCKEV_B2_SB(res5, res4, res7, res6, tmp2, tmp3);\n+    tmp0 = __msa_aver_s_b(tmp0, src0);\n+    tmp1 = __msa_aver_s_b(tmp1, src1);\n+    tmp2 = __msa_aver_s_b(tmp2, src4);\n+    tmp3 = __msa_aver_s_b(tmp3, src5);\n+    XORI_B4_128_SB(tmp0, tmp1, tmp2, tmp3);\n+    ST8x8_UB(tmp0, tmp1, tmp2, tmp3, dst, stride);\n }\n \n void ff_put_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,\n                                 ptrdiff_t stride)\n {\n-    avc_luma_hz_qrt_4w_msa(src - 2, stride, dst, stride, 4, 0);\n+    v16i8 src0, src1, src2, src3, res, mask0, mask1, mask2;\n+    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n+    v8i16 res0, res1;\n+    v16i8 minus5b = __msa_ldi_b(-5);\n+    v16i8 plus20b = __msa_ldi_b(20);\n+\n+    LD_SB3(&luma_mask_arr[48], 16, mask0, mask1, mask2);\n+    LD_SB4(src - 2, stride, src0, src1, src2, src3);\n+    XORI_B4_128_SB(src0, src1, src2, src3);\n+    VSHF_B2_SB(src0, src1, src2, src3, mask0, mask0, vec0, vec1);\n+    HADD_SB2_SH(vec0, vec1, res0, res1);\n+    VSHF_B2_SB(src0, src1, src2, src3, mask1, mask1, vec2, vec3);\n+    DPADD_SB2_SH(vec2, vec3, minus5b, minus5b, res0, res1);\n+    VSHF_B2_SB(src0, src1, src2, src3, mask2, mask2, vec4, vec5);\n+    DPADD_SB2_SH(vec4, vec5, plus20b, plus20b, res0, res1);\n+    SRARI_H2_SH(res0, res1, 5);\n+    SAT_SH2_SH(res0, res1, 7);\n+    res = __msa_pckev_b((v16i8) res1, (v16i8) res0);\n+    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);\n+    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);\n+    src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);\n+    src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);\n+    src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);\n+    res = __msa_aver_s_b(res, src0);\n+    res = (v16i8) __msa_xori_b((v16u8) res, 128);\n+    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);\n }\n \n void ff_put_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,\n                                 ptrdiff_t stride)\n {\n-    avc_luma_hz_qrt_4w_msa(src - 2, stride, dst, stride, 4, 1);\n+    v16i8 src0, src1, src2, src3, res, mask0, mask1, mask2;\n+    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n+    v8i16 res0, res1;\n+    v16i8 minus5b = __msa_ldi_b(-5);\n+    v16i8 plus20b = __msa_ldi_b(20);\n+\n+    LD_SB3(&luma_mask_arr[48], 16, mask0, mask1, mask2);\n+    LD_SB4(src - 2, stride, src0, src1, src2, src3);\n+    XORI_B4_128_SB(src0, src1, src2, src3);\n+    VSHF_B2_SB(src0, src1, src2, src3, mask0, mask0, vec0, vec1);\n+    HADD_SB2_SH(vec0, vec1, res0, res1);\n+    VSHF_B2_SB(src0, src1, src2, src3, mask1, mask1, vec2, vec3);\n+    DPADD_SB2_SH(vec2, vec3, minus5b, minus5b, res0, res1);\n+    VSHF_B2_SB(src0, src1, src2, src3, mask2, mask2, vec4, vec5);\n+    DPADD_SB2_SH(vec4, vec5, plus20b, plus20b, res0, res1);\n+    SRARI_H2_SH(res0, res1, 5);\n+    SAT_SH2_SH(res0, res1, 7);\n+    res = __msa_pckev_b((v16i8) res1, (v16i8) res0);\n+    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);\n+    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);\n+    src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);\n+    src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);\n+    src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);\n+    res = __msa_aver_s_b(res, src0);\n+    res = (v16i8) __msa_xori_b((v16u8) res, 128);\n+    ST4x4_UB(res, res, 0, 1, 2, 3, dst, stride);\n }\n \n void ff_put_h264_qpel16_mc20_msa(uint8_t *dst, const uint8_t *src,\n", "ori_dataset": "ffmpeg", "commit_id": "b5da07d434", "dependency": {"avc_luma_hz_qrt_4w_msa": "static void avc_luma_hz_qrt_4w_msa(const uint8_t *src, int32_t src_stride,\n                                   uint8_t *dst, int32_t dst_stride,\n                                   int32_t height, uint8_t hor_offset)\n{\n    uint8_t slide;\n    uint32_t loop_cnt;\n    v16i8 src0, src1, src2, src3;\n    v8i16 res0, res1;\n    v16i8 res, mask0, mask1, mask2;\n    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n    v16i8 minus5b = __msa_ldi_b(-5);\n    v16i8 plus20b = __msa_ldi_b(20);\n\n    LD_SB3(&luma_mask_arr[48], 16, mask0, mask1, mask2);\n    slide = 2 + hor_offset;\n\n    for (loop_cnt = (height >> 2); loop_cnt--;) {\n        LD_SB4(src, src_stride, src0, src1, src2, src3);\n        src += (4 * src_stride);\n\n        XORI_B4_128_SB(src0, src1, src2, src3);\n        VSHF_B2_SB(src0, src1, src2, src3, mask0, mask0, vec0, vec1);\n        HADD_SB2_SH(vec0, vec1, res0, res1);\n        VSHF_B2_SB(src0, src1, src2, src3, mask1, mask1, vec2, vec3);\n        DPADD_SB2_SH(vec2, vec3, minus5b, minus5b, res0, res1);\n        VSHF_B2_SB(src0, src1, src2, src3, mask2, mask2, vec4, vec5);\n        DPADD_SB2_SH(vec4, vec5, plus20b, plus20b, res0, res1);\n        SRARI_H2_SH(res0, res1, 5);\n        SAT_SH2_SH(res0, res1, 7);\n\n        res = __msa_pckev_b((v16i8) res1, (v16i8) res0);\n        src0 = __msa_sld_b(src0, src0, slide);\n        src1 = __msa_sld_b(src1, src1, slide);\n        src2 = __msa_sld_b(src2, src2, slide);\n        src3 = __msa_sld_b(src3, src3, slide);\n        src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);\n        src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);\n        src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);\n        res = __msa_aver_s_b(res, src0);\n        res = (v16i8) __msa_xori_b((v16u8) res, 128);\n\n        ST4x4_UB(res, res, 0, 1, 2, 3, dst, dst_stride);\n        dst += (4 * dst_stride);\n    }\n}", "avc_luma_hz_qrt_8w_msa": "static void avc_luma_hz_qrt_8w_msa(const uint8_t *src, int32_t src_stride,\n                                   uint8_t *dst, int32_t dst_stride,\n                                   int32_t height, uint8_t hor_offset)\n{\n    uint8_t slide;\n    uint32_t loop_cnt;\n    v16i8 src0, src1, src2, src3;\n    v16i8 tmp0, tmp1;\n    v8i16 res0, res1, res2, res3;\n    v16i8 mask0, mask1, mask2;\n    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n    v16i8 vec6, vec7, vec8, vec9, vec10, vec11;\n    v16i8 minus5b = __msa_ldi_b(-5);\n    v16i8 plus20b = __msa_ldi_b(20);\n\n    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n    slide = 2 + hor_offset;\n\n    for (loop_cnt = height >> 2; loop_cnt--;) {\n        LD_SB4(src, src_stride, src0, src1, src2, src3);\n        src += (4 * src_stride);\n\n        XORI_B4_128_SB(src0, src1, src2, src3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec0, vec1);\n        VSHF_B2_SB(src2, src2, src3, src3, mask0, mask0, vec2, vec3);\n        HADD_SB4_SH(vec0, vec1, vec2, vec3, res0, res1, res2, res3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask1, mask1, vec4, vec5);\n        VSHF_B2_SB(src2, src2, src3, src3, mask1, mask1, vec6, vec7);\n        DPADD_SB4_SH(vec4, vec5, vec6, vec7, minus5b, minus5b, minus5b, minus5b,\n                     res0, res1, res2, res3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask2, mask2, vec8, vec9);\n        VSHF_B2_SB(src2, src2, src3, src3, mask2, mask2, vec10, vec11);\n        DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b,\n                     plus20b, res0, res1, res2, res3);\n\n        src0 = __msa_sld_b(src0, src0, slide);\n        src1 = __msa_sld_b(src1, src1, slide);\n        src2 = __msa_sld_b(src2, src2, slide);\n        src3 = __msa_sld_b(src3, src3, slide);\n\n        SRARI_H4_SH(res0, res1, res2, res3, 5);\n        SAT_SH4_SH(res0, res1, res2, res3, 7);\n        PCKEV_B2_SB(res1, res0, res3, res2, tmp0, tmp1);\n        PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);\n\n        tmp0 = __msa_aver_s_b(tmp0, src0);\n        tmp1 = __msa_aver_s_b(tmp1, src1);\n\n        XORI_B2_128_SB(tmp0, tmp1);\n        ST8x4_UB(tmp0, tmp1, dst, dst_stride);\n\n        dst += (4 * dst_stride);\n    }\n}", "avc_luma_hz_qrt_16w_msa": "static void avc_luma_hz_qrt_16w_msa(const uint8_t *src, int32_t src_stride,\n                                    uint8_t *dst, int32_t dst_stride,\n                                    int32_t height, uint8_t hor_offset)\n{\n    uint32_t loop_cnt;\n    v16i8 dst0, dst1;\n    v16i8 src0, src1, src2, src3;\n    v16i8 mask0, mask1, mask2, vshf;\n    v8i16 res0, res1, res2, res3;\n    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n    v16i8 vec6, vec7, vec8, vec9, vec10, vec11;\n    v16i8 minus5b = __msa_ldi_b(-5);\n    v16i8 plus20b = __msa_ldi_b(20);\n\n    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n\n    if (hor_offset) {\n        vshf = LD_SB(&luma_mask_arr[16 + 96]);\n    } else {\n        vshf = LD_SB(&luma_mask_arr[96]);\n    }\n\n    for (loop_cnt = height >> 1; loop_cnt--;) {\n        LD_SB2(src, 8, src0, src1);\n        src += src_stride;\n        LD_SB2(src, 8, src2, src3);\n        src += src_stride;\n\n        XORI_B4_128_SB(src0, src1, src2, src3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec0, vec3);\n        VSHF_B2_SB(src2, src2, src3, src3, mask0, mask0, vec6, vec9);\n        VSHF_B2_SB(src0, src0, src1, src1, mask1, mask1, vec1, vec4);\n        VSHF_B2_SB(src2, src2, src3, src3, mask1, mask1, vec7, vec10);\n        VSHF_B2_SB(src0, src0, src1, src1, mask2, mask2, vec2, vec5);\n        VSHF_B2_SB(src2, src2, src3, src3, mask2, mask2, vec8, vec11);\n        HADD_SB4_SH(vec0, vec3, vec6, vec9, res0, res1, res2, res3);\n        DPADD_SB4_SH(vec1, vec4, vec7, vec10, minus5b, minus5b, minus5b,\n                     minus5b, res0, res1, res2, res3);\n        DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,\n                     plus20b, res0, res1, res2, res3);\n        VSHF_B2_SB(src0, src1, src2, src3, vshf, vshf, src0, src2);\n        SRARI_H4_SH(res0, res1, res2, res3, 5);\n        SAT_SH4_SH(res0, res1, res2, res3, 7);\n        PCKEV_B2_SB(res1, res0, res3, res2, dst0, dst1);\n\n        dst0 = __msa_aver_s_b(dst0, src0);\n        dst1 = __msa_aver_s_b(dst1, src2);\n\n        XORI_B2_128_SB(dst0, dst1);\n\n        ST_SB2(dst0, dst1, dst, dst_stride);\n        dst += (2 * dst_stride);\n    }\n}", "LD_SB2": "", "LD_SB3": "", "ST8x8_UB": "", "DPADD_SB2_SH": "", "SAT_SH4_SH": "", "LD_SB4": "", "SRARI_H4_SH": "", "XORI_B4_128_SB": "", "SAT_SH2_SH": "", "PCKEV_B2_SB": "", "HADD_SB2_SH": "", "PCKEV_D2_SB": "", "ST4x4_UB": "", "XORI_B8_128_SB": "", "LD_SB8": "", "HADD_SB4_SH": "", "SLDI_B2_SB": "", "ST_SB4": "", "DPADD_SB4_SH": "", "VSHF_B2_SB": "", "SRARI_H2_SH": ""}, "pre_dep": {"avc_luma_hz_qrt_4w_msa": "static void avc_luma_hz_qrt_4w_msa(const uint8_t *src, int32_t src_stride,\n                                   uint8_t *dst, int32_t dst_stride,\n                                   int32_t height, uint8_t hor_offset)\n{\n    uint8_t slide;\n    uint32_t loop_cnt;\n    v16i8 src0, src1, src2, src3;\n    v8i16 res0, res1;\n    v16i8 res, mask0, mask1, mask2;\n    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n    v16i8 minus5b = __msa_ldi_b(-5);\n    v16i8 plus20b = __msa_ldi_b(20);\n\n    LD_SB3(&luma_mask_arr[48], 16, mask0, mask1, mask2);\n    slide = 2 + hor_offset;\n\n    for (loop_cnt = (height >> 2); loop_cnt--;) {\n        LD_SB4(src, src_stride, src0, src1, src2, src3);\n        src += (4 * src_stride);\n\n        XORI_B4_128_SB(src0, src1, src2, src3);\n        VSHF_B2_SB(src0, src1, src2, src3, mask0, mask0, vec0, vec1);\n        HADD_SB2_SH(vec0, vec1, res0, res1);\n        VSHF_B2_SB(src0, src1, src2, src3, mask1, mask1, vec2, vec3);\n        DPADD_SB2_SH(vec2, vec3, minus5b, minus5b, res0, res1);\n        VSHF_B2_SB(src0, src1, src2, src3, mask2, mask2, vec4, vec5);\n        DPADD_SB2_SH(vec4, vec5, plus20b, plus20b, res0, res1);\n        SRARI_H2_SH(res0, res1, 5);\n        SAT_SH2_SH(res0, res1, 7);\n\n        res = __msa_pckev_b((v16i8) res1, (v16i8) res0);\n        src0 = __msa_sld_b(src0, src0, slide);\n        src1 = __msa_sld_b(src1, src1, slide);\n        src2 = __msa_sld_b(src2, src2, slide);\n        src3 = __msa_sld_b(src3, src3, slide);\n        src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);\n        src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);\n        src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);\n        res = __msa_aver_s_b(res, src0);\n        res = (v16i8) __msa_xori_b((v16u8) res, 128);\n\n        ST4x4_UB(res, res, 0, 1, 2, 3, dst, dst_stride);\n        dst += (4 * dst_stride);\n    }\n}", "avc_luma_hz_qrt_8w_msa": "static void avc_luma_hz_qrt_8w_msa(const uint8_t *src, int32_t src_stride,\n                                   uint8_t *dst, int32_t dst_stride,\n                                   int32_t height, uint8_t hor_offset)\n{\n    uint8_t slide;\n    uint32_t loop_cnt;\n    v16i8 src0, src1, src2, src3;\n    v16i8 tmp0, tmp1;\n    v8i16 res0, res1, res2, res3;\n    v16i8 mask0, mask1, mask2;\n    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n    v16i8 vec6, vec7, vec8, vec9, vec10, vec11;\n    v16i8 minus5b = __msa_ldi_b(-5);\n    v16i8 plus20b = __msa_ldi_b(20);\n\n    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n    slide = 2 + hor_offset;\n\n    for (loop_cnt = height >> 2; loop_cnt--;) {\n        LD_SB4(src, src_stride, src0, src1, src2, src3);\n        src += (4 * src_stride);\n\n        XORI_B4_128_SB(src0, src1, src2, src3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec0, vec1);\n        VSHF_B2_SB(src2, src2, src3, src3, mask0, mask0, vec2, vec3);\n        HADD_SB4_SH(vec0, vec1, vec2, vec3, res0, res1, res2, res3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask1, mask1, vec4, vec5);\n        VSHF_B2_SB(src2, src2, src3, src3, mask1, mask1, vec6, vec7);\n        DPADD_SB4_SH(vec4, vec5, vec6, vec7, minus5b, minus5b, minus5b, minus5b,\n                     res0, res1, res2, res3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask2, mask2, vec8, vec9);\n        VSHF_B2_SB(src2, src2, src3, src3, mask2, mask2, vec10, vec11);\n        DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b,\n                     plus20b, res0, res1, res2, res3);\n\n        src0 = __msa_sld_b(src0, src0, slide);\n        src1 = __msa_sld_b(src1, src1, slide);\n        src2 = __msa_sld_b(src2, src2, slide);\n        src3 = __msa_sld_b(src3, src3, slide);\n\n        SRARI_H4_SH(res0, res1, res2, res3, 5);\n        SAT_SH4_SH(res0, res1, res2, res3, 7);\n        PCKEV_B2_SB(res1, res0, res3, res2, tmp0, tmp1);\n        PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);\n\n        tmp0 = __msa_aver_s_b(tmp0, src0);\n        tmp1 = __msa_aver_s_b(tmp1, src1);\n\n        XORI_B2_128_SB(tmp0, tmp1);\n        ST8x4_UB(tmp0, tmp1, dst, dst_stride);\n\n        dst += (4 * dst_stride);\n    }\n}", "avc_luma_hz_qrt_16w_msa": "static void avc_luma_hz_qrt_16w_msa(const uint8_t *src, int32_t src_stride,\n                                    uint8_t *dst, int32_t dst_stride,\n                                    int32_t height, uint8_t hor_offset)\n{\n    uint32_t loop_cnt;\n    v16i8 dst0, dst1;\n    v16i8 src0, src1, src2, src3;\n    v16i8 mask0, mask1, mask2, vshf;\n    v8i16 res0, res1, res2, res3;\n    v16i8 vec0, vec1, vec2, vec3, vec4, vec5;\n    v16i8 vec6, vec7, vec8, vec9, vec10, vec11;\n    v16i8 minus5b = __msa_ldi_b(-5);\n    v16i8 plus20b = __msa_ldi_b(20);\n\n    LD_SB3(&luma_mask_arr[0], 16, mask0, mask1, mask2);\n\n    if (hor_offset) {\n        vshf = LD_SB(&luma_mask_arr[16 + 96]);\n    } else {\n        vshf = LD_SB(&luma_mask_arr[96]);\n    }\n\n    for (loop_cnt = height >> 1; loop_cnt--;) {\n        LD_SB2(src, 8, src0, src1);\n        src += src_stride;\n        LD_SB2(src, 8, src2, src3);\n        src += src_stride;\n\n        XORI_B4_128_SB(src0, src1, src2, src3);\n        VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec0, vec3);\n        VSHF_B2_SB(src2, src2, src3, src3, mask0, mask0, vec6, vec9);\n        VSHF_B2_SB(src0, src0, src1, src1, mask1, mask1, vec1, vec4);\n        VSHF_B2_SB(src2, src2, src3, src3, mask1, mask1, vec7, vec10);\n        VSHF_B2_SB(src0, src0, src1, src1, mask2, mask2, vec2, vec5);\n        VSHF_B2_SB(src2, src2, src3, src3, mask2, mask2, vec8, vec11);\n        HADD_SB4_SH(vec0, vec3, vec6, vec9, res0, res1, res2, res3);\n        DPADD_SB4_SH(vec1, vec4, vec7, vec10, minus5b, minus5b, minus5b,\n                     minus5b, res0, res1, res2, res3);\n        DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,\n                     plus20b, res0, res1, res2, res3);\n        VSHF_B2_SB(src0, src1, src2, src3, vshf, vshf, src0, src2);\n        SRARI_H4_SH(res0, res1, res2, res3, 5);\n        SAT_SH4_SH(res0, res1, res2, res3, 7);\n        PCKEV_B2_SB(res1, res0, res3, res2, dst0, dst1);\n\n        dst0 = __msa_aver_s_b(dst0, src0);\n        dst1 = __msa_aver_s_b(dst1, src2);\n\n        XORI_B2_128_SB(dst0, dst1);\n\n        ST_SB2(dst0, dst1, dst, dst_stride);\n        dst += (2 * dst_stride);\n    }\n}"}, "post_dep": {}}
{"idx": 11, "category": "non-security", "commit_message": " lavf/tls_gnutls: fix warnings from version check The GnuTLS version is checked through the macro GNUTLS_VERSION_NUMBER,&&&&but this wasn't introduced before 2.7.2. Building with older versions&&&&of GnuTLS (using icc) warns:&&&&&&&&src/libavformat/tls_gnutls.c(38): warning #193: zero used for undefined preprocessing identifier \"GNUTLS_VERSION_NUMBER\"&&&&  #if HAVE_THREADS && GNUTLS_VERSION_NUMBER <= 0x020b00&&&&&&&&This adds a fallback to the older, deprecated LIBGNUTLS_VERSION_NUMBER&&&&macro.&&&&&&&&Signed-off-by: Moritz Barsnick <barsnick@gmx.net>&&&& ", "diff_code": "diff --git a/libavformat/tls_gnutls.c b/libavformat/tls_gnutls.c\nindex 7174dfd..5ce6c3d 100644\n--- a/libavformat/tls_gnutls.c\n+++ b/libavformat/tls_gnutls.c\n@@ -35,6 +35,10 @@\n #include \"libavutil/opt.h\"\n #include \"libavutil/parseutils.h\"\n \n+#ifndef GNUTLS_VERSION_NUMBER\n+#define GNUTLS_VERSION_NUMBER LIBGNUTLS_VERSION_NUMBER\n+#endif\n+\n #if HAVE_THREADS && GNUTLS_VERSION_NUMBER <= 0x020b00\n #include <gcrypt.h>\n #include \"libavutil/thread.h\"\n", "ori_dataset": "ffmpeg", "commit_id": "6bf48c4805", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 12, "category": "non-security", "commit_message": " avcodec/mips: Improve avc mc copy msa functions Remove loops and unroll as block sizes are known.&&&&&&&&Signed-off-by: Kaustubh Raste <kaustubh.raste@imgtec.com>&&&&Reviewed-by: Manojkumar Bhosale <Manojkumar.Bhosale@imgtec.com>&&&&Signed-off-by: Michael Niedermayer <michael@niedermayer.cc>&&&& ", "diff_code": "diff --git a/libavcodec/mips/h264qpel_msa.c b/libavcodec/mips/h264qpel_msa.c\nindex 43d21f7..05dffea 100644\n--- a/libavcodec/mips/h264qpel_msa.c\n+++ b/libavcodec/mips/h264qpel_msa.c\n@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2015 Parag Salasakar (Parag.Salasakar@imgtec.com)\n+ * Copyright (c) 2015 -2017 Parag Salasakar (Parag.Salasakar@imgtec.com)\n  *\n  * This file is part of FFmpeg.\n  *\n@@ -2966,31 +2966,100 @@ static void avg_width16_msa(const uint8_t *src, int32_t src_stride,\n void ff_put_h264_qpel16_mc00_msa(uint8_t *dst, const uint8_t *src,\n                                  ptrdiff_t stride)\n {\n-    copy_width16_msa(src, stride, dst, stride, 16);\n+    v16u8 src0, src1, src2, src3, src4, src5, src6, src7;\n+    v16u8 src8, src9, src10, src11, src12, src13, src14, src15;\n+\n+    LD_UB8(src, stride, src0, src1, src2, src3, src4, src5, src6, src7);\n+    src += (8 * stride);\n+    LD_UB8(src, stride, src8, src9, src10, src11, src12, src13, src14, src15);\n+\n+    ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, stride);\n+    dst += (8 * stride);\n+    ST_UB8(src8, src9, src10, src11, src12, src13, src14, src15, dst, stride);\n }\n \n void ff_put_h264_qpel8_mc00_msa(uint8_t *dst, const uint8_t *src,\n                                 ptrdiff_t stride)\n {\n-    copy_width8_msa(src, stride, dst, stride, 8);\n+    uint64_t src0, src1, src2, src3, src4, src5, src6, src7;\n+\n+    LD4(src, stride, src0, src1, src2, src3);\n+    src += 4 * stride;\n+    LD4(src, stride, src4, src5, src6, src7);\n+    SD4(src0, src1, src2, src3, dst, stride);\n+    dst += 4 * stride;\n+    SD4(src4, src5, src6, src7, dst, stride);\n }\n \n void ff_avg_h264_qpel16_mc00_msa(uint8_t *dst, const uint8_t *src,\n                                  ptrdiff_t stride)\n {\n-    avg_width16_msa(src, stride, dst, stride, 16);\n+    v16u8 src0, src1, src2, src3, src4, src5, src6, src7;\n+    v16u8 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;\n+\n+    LD_UB8(src, stride, src0, src1, src2, src3, src4, src5, src6, src7);\n+    src += (8 * stride);\n+    LD_UB8(dst, stride, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);\n+\n+    AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0, dst1,\n+                dst2, dst3);\n+    AVER_UB4_UB(src4, dst4, src5, dst5, src6, dst6, src7, dst7, dst4, dst5,\n+                dst6, dst7);\n+    ST_UB8(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst, stride);\n+    dst += (8 * stride);\n+\n+    LD_UB8(src, stride, src0, src1, src2, src3, src4, src5, src6, src7);\n+    LD_UB8(dst, stride, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);\n+\n+    AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0, dst1,\n+                dst2, dst3);\n+    AVER_UB4_UB(src4, dst4, src5, dst5, src6, dst6, src7, dst7, dst4, dst5,\n+                dst6, dst7);\n+    ST_UB8(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst, stride);\n }\n \n void ff_avg_h264_qpel8_mc00_msa(uint8_t *dst, const uint8_t *src,\n                                 ptrdiff_t stride)\n {\n-    avg_width8_msa(src, stride, dst, stride, 8);\n+    uint64_t tp0, tp1, tp2, tp3, tp4, tp5, tp6, tp7;\n+    v16u8 src0 = { 0 }, src1 = { 0 }, src2 = { 0 }, src3 = { 0 };\n+    v16u8 dst0 = { 0 }, dst1 = { 0 }, dst2 = { 0 }, dst3 = { 0 };\n+\n+    LD4(src, stride, tp0, tp1, tp2, tp3);\n+    src += 4 * stride;\n+    LD4(src, stride, tp4, tp5, tp6, tp7);\n+    INSERT_D2_UB(tp0, tp1, src0);\n+    INSERT_D2_UB(tp2, tp3, src1);\n+    INSERT_D2_UB(tp4, tp5, src2);\n+    INSERT_D2_UB(tp6, tp7, src3);\n+\n+    LD4(dst, stride, tp0, tp1, tp2, tp3);\n+    LD4(dst + 4 * stride, stride, tp4, tp5, tp6, tp7);\n+    INSERT_D2_UB(tp0, tp1, dst0);\n+    INSERT_D2_UB(tp2, tp3, dst1);\n+    INSERT_D2_UB(tp4, tp5, dst2);\n+    INSERT_D2_UB(tp6, tp7, dst3);\n+\n+    AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0, dst1,\n+                dst2, dst3);\n+\n+    ST8x8_UB(dst0, dst1, dst2, dst3, dst, stride);\n }\n \n void ff_avg_h264_qpel4_mc00_msa(uint8_t *dst, const uint8_t *src,\n                                 ptrdiff_t stride)\n {\n-    avg_width4_msa(src, stride, dst, stride, 4);\n+    uint32_t tp0, tp1, tp2, tp3;\n+    v16u8 src0 = { 0 }, dst0 = { 0 };\n+\n+    LW4(src, stride, tp0, tp1, tp2, tp3);\n+    INSERT_W4_UB(tp0, tp1, tp2, tp3, src0);\n+    LW4(dst, stride, tp0, tp1, tp2, tp3);\n+    INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);\n+\n+    dst0 = __msa_aver_u_b(src0, dst0);\n+\n+    ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, stride);\n }\n \n void ff_put_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,\n", "ori_dataset": "ffmpeg", "commit_id": "0105ed551c", "dependency": {"copy_width16_msa": "static void copy_width16_msa(const uint8_t *src, int32_t src_stride,\n                             uint8_t *dst, int32_t dst_stride,\n                             int32_t height)\n{\n    int32_t cnt;\n    v16u8 src0, src1, src2, src3, src4, src5, src6, src7;\n\n    if (8 == height) {\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n    } else if (16 == height) {\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n    } else if (32 == height) {\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n    } else if (0 == height % 4) {\n        for (cnt = (height >> 2); cnt--;) {\n            LD_UB4(src, src_stride, src0, src1, src2, src3);\n            src += (4 * src_stride);\n            ST_UB4(src0, src1, src2, src3, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    }\n}", "copy_width8_msa": "static void copy_width8_msa(const uint8_t *src, int32_t src_stride,\n                            uint8_t *dst, int32_t dst_stride,\n                            int32_t height)\n{\n    int32_t cnt;\n    uint64_t out0, out1, out2, out3, out4, out5, out6, out7;\n\n    if (0 == height % 8) {\n        for (cnt = height >> 3; cnt--;) {\n            LD4(src, src_stride, out0, out1, out2, out3);\n            src += (4 * src_stride);\n            LD4(src, src_stride, out4, out5, out6, out7);\n            src += (4 * src_stride);\n\n            SD4(out0, out1, out2, out3, dst, dst_stride);\n            dst += (4 * dst_stride);\n            SD4(out4, out5, out6, out7, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    } else if (0 == height % 4) {\n        for (cnt = (height / 4); cnt--;) {\n            LD4(src, src_stride, out0, out1, out2, out3);\n            src += (4 * src_stride);\n\n            SD4(out0, out1, out2, out3, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    }\n}", "avg_width4_msa": "static void avg_width4_msa(const uint8_t *src, int32_t src_stride,\n                           uint8_t *dst, int32_t dst_stride,\n                           int32_t height)\n{\n    uint32_t tp0, tp1, tp2, tp3;\n    v16u8 src0 = { 0 }, src1 = { 0 }, dst0 = { 0 }, dst1 = { 0 };\n\n    if (8 == height) {\n        LW4(src, src_stride, tp0, tp1, tp2, tp3);\n        src += 4 * src_stride;\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, src0);\n        LW4(src, src_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, src1);\n        LW4(dst, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);\n        LW4(dst + 4 * dst_stride, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, dst1);\n        AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);\n        ST4x8_UB(dst0, dst1, dst, dst_stride);\n    } else if (4 == height) {\n        LW4(src, src_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, src0);\n        LW4(dst, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);\n        dst0 = __msa_aver_u_b(src0, dst0);\n        ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);\n    }\n}", "avg_width8_msa": "static void avg_width8_msa(const uint8_t *src, int32_t src_stride,\n                           uint8_t *dst, int32_t dst_stride,\n                           int32_t height)\n{\n    int32_t cnt;\n    uint64_t tp0, tp1, tp2, tp3, tp4, tp5, tp6, tp7;\n    v16u8 src0, src1, src2, src3;\n    v16u8 dst0, dst1, dst2, dst3;\n\n    if (0 == (height % 8)) {\n        for (cnt = (height >> 3); cnt--;) {\n            LD4(src, src_stride, tp0, tp1, tp2, tp3);\n            src += 4 * src_stride;\n            LD4(src, src_stride, tp4, tp5, tp6, tp7);\n            src += 4 * src_stride;\n            INSERT_D2_UB(tp0, tp1, src0);\n            INSERT_D2_UB(tp2, tp3, src1);\n            INSERT_D2_UB(tp4, tp5, src2);\n            INSERT_D2_UB(tp6, tp7, src3);\n            LD4(dst, dst_stride, tp0, tp1, tp2, tp3);\n            LD4(dst + 4 * dst_stride, dst_stride, tp4, tp5, tp6, tp7);\n            INSERT_D2_UB(tp0, tp1, dst0);\n            INSERT_D2_UB(tp2, tp3, dst1);\n            INSERT_D2_UB(tp4, tp5, dst2);\n            INSERT_D2_UB(tp6, tp7, dst3);\n            AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0,\n                        dst1, dst2, dst3);\n            ST8x8_UB(dst0, dst1, dst2, dst3, dst, dst_stride);\n            dst += 8 * dst_stride;\n        }\n    } else if (4 == height) {\n        LD4(src, src_stride, tp0, tp1, tp2, tp3);\n        INSERT_D2_UB(tp0, tp1, src0);\n        INSERT_D2_UB(tp2, tp3, src1);\n        LD4(dst, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_D2_UB(tp0, tp1, dst0);\n        INSERT_D2_UB(tp2, tp3, dst1);\n        AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);\n        ST8x4_UB(dst0, dst1, dst, dst_stride);\n    }\n}", "avg_width16_msa": "static void avg_width16_msa(const uint8_t *src, int32_t src_stride,\n                            uint8_t *dst, int32_t dst_stride,\n                            int32_t height)\n{\n    int32_t cnt;\n    v16u8 src0, src1, src2, src3, src4, src5, src6, src7;\n    v16u8 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;\n\n    if (0 == (height % 8)) {\n        for (cnt = (height / 8); cnt--;) {\n            LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n            src += (8 * src_stride);\n            LD_UB8(dst, dst_stride, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);\n\n            AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3,\n                        dst0, dst1, dst2, dst3);\n            AVER_UB4_UB(src4, dst4, src5, dst5, src6, dst6, src7, dst7,\n                        dst4, dst5, dst6, dst7);\n            ST_UB8(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst, dst_stride);\n            dst += (8 * dst_stride);\n        }\n    } else if (0 == (height % 4)) {\n        for (cnt = (height / 4); cnt--;) {\n            LD_UB4(src, src_stride, src0, src1, src2, src3);\n            src += (4 * src_stride);\n            LD_UB4(dst, dst_stride, dst0, dst1, dst2, dst3);\n\n            AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3,\n                        dst0, dst1, dst2, dst3);\n            ST_UB4(dst0, dst1, dst2, dst3, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    }\n}", "AVER_UB4_UB": "", "LD4": "", "INSERT_W4_UB": "", "SD4": "", "LD_UB8": "", "INSERT_D2_UB": "", "LW4": "", "ST8x8_UB": "", "ST_UB8": "", "ST4x4_UB": ""}, "pre_dep": {"copy_width16_msa": "static void copy_width16_msa(const uint8_t *src, int32_t src_stride,\n                             uint8_t *dst, int32_t dst_stride,\n                             int32_t height)\n{\n    int32_t cnt;\n    v16u8 src0, src1, src2, src3, src4, src5, src6, src7;\n\n    if (8 == height) {\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n    } else if (16 == height) {\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n    } else if (32 == height) {\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        src += (8 * src_stride);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n        dst += (8 * dst_stride);\n        LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n        ST_UB8(src0, src1, src2, src3, src4, src5, src6, src7, dst, dst_stride);\n    } else if (0 == height % 4) {\n        for (cnt = (height >> 2); cnt--;) {\n            LD_UB4(src, src_stride, src0, src1, src2, src3);\n            src += (4 * src_stride);\n            ST_UB4(src0, src1, src2, src3, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    }\n}", "copy_width8_msa": "static void copy_width8_msa(const uint8_t *src, int32_t src_stride,\n                            uint8_t *dst, int32_t dst_stride,\n                            int32_t height)\n{\n    int32_t cnt;\n    uint64_t out0, out1, out2, out3, out4, out5, out6, out7;\n\n    if (0 == height % 8) {\n        for (cnt = height >> 3; cnt--;) {\n            LD4(src, src_stride, out0, out1, out2, out3);\n            src += (4 * src_stride);\n            LD4(src, src_stride, out4, out5, out6, out7);\n            src += (4 * src_stride);\n\n            SD4(out0, out1, out2, out3, dst, dst_stride);\n            dst += (4 * dst_stride);\n            SD4(out4, out5, out6, out7, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    } else if (0 == height % 4) {\n        for (cnt = (height / 4); cnt--;) {\n            LD4(src, src_stride, out0, out1, out2, out3);\n            src += (4 * src_stride);\n\n            SD4(out0, out1, out2, out3, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    }\n}", "avg_width4_msa": "static void avg_width4_msa(const uint8_t *src, int32_t src_stride,\n                           uint8_t *dst, int32_t dst_stride,\n                           int32_t height)\n{\n    uint32_t tp0, tp1, tp2, tp3;\n    v16u8 src0 = { 0 }, src1 = { 0 }, dst0 = { 0 }, dst1 = { 0 };\n\n    if (8 == height) {\n        LW4(src, src_stride, tp0, tp1, tp2, tp3);\n        src += 4 * src_stride;\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, src0);\n        LW4(src, src_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, src1);\n        LW4(dst, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);\n        LW4(dst + 4 * dst_stride, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, dst1);\n        AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);\n        ST4x8_UB(dst0, dst1, dst, dst_stride);\n    } else if (4 == height) {\n        LW4(src, src_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, src0);\n        LW4(dst, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_W4_UB(tp0, tp1, tp2, tp3, dst0);\n        dst0 = __msa_aver_u_b(src0, dst0);\n        ST4x4_UB(dst0, dst0, 0, 1, 2, 3, dst, dst_stride);\n    }\n}", "avg_width8_msa": "static void avg_width8_msa(const uint8_t *src, int32_t src_stride,\n                           uint8_t *dst, int32_t dst_stride,\n                           int32_t height)\n{\n    int32_t cnt;\n    uint64_t tp0, tp1, tp2, tp3, tp4, tp5, tp6, tp7;\n    v16u8 src0, src1, src2, src3;\n    v16u8 dst0, dst1, dst2, dst3;\n\n    if (0 == (height % 8)) {\n        for (cnt = (height >> 3); cnt--;) {\n            LD4(src, src_stride, tp0, tp1, tp2, tp3);\n            src += 4 * src_stride;\n            LD4(src, src_stride, tp4, tp5, tp6, tp7);\n            src += 4 * src_stride;\n            INSERT_D2_UB(tp0, tp1, src0);\n            INSERT_D2_UB(tp2, tp3, src1);\n            INSERT_D2_UB(tp4, tp5, src2);\n            INSERT_D2_UB(tp6, tp7, src3);\n            LD4(dst, dst_stride, tp0, tp1, tp2, tp3);\n            LD4(dst + 4 * dst_stride, dst_stride, tp4, tp5, tp6, tp7);\n            INSERT_D2_UB(tp0, tp1, dst0);\n            INSERT_D2_UB(tp2, tp3, dst1);\n            INSERT_D2_UB(tp4, tp5, dst2);\n            INSERT_D2_UB(tp6, tp7, dst3);\n            AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3, dst0,\n                        dst1, dst2, dst3);\n            ST8x8_UB(dst0, dst1, dst2, dst3, dst, dst_stride);\n            dst += 8 * dst_stride;\n        }\n    } else if (4 == height) {\n        LD4(src, src_stride, tp0, tp1, tp2, tp3);\n        INSERT_D2_UB(tp0, tp1, src0);\n        INSERT_D2_UB(tp2, tp3, src1);\n        LD4(dst, dst_stride, tp0, tp1, tp2, tp3);\n        INSERT_D2_UB(tp0, tp1, dst0);\n        INSERT_D2_UB(tp2, tp3, dst1);\n        AVER_UB2_UB(src0, dst0, src1, dst1, dst0, dst1);\n        ST8x4_UB(dst0, dst1, dst, dst_stride);\n    }\n}", "avg_width16_msa": "static void avg_width16_msa(const uint8_t *src, int32_t src_stride,\n                            uint8_t *dst, int32_t dst_stride,\n                            int32_t height)\n{\n    int32_t cnt;\n    v16u8 src0, src1, src2, src3, src4, src5, src6, src7;\n    v16u8 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;\n\n    if (0 == (height % 8)) {\n        for (cnt = (height / 8); cnt--;) {\n            LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);\n            src += (8 * src_stride);\n            LD_UB8(dst, dst_stride, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);\n\n            AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3,\n                        dst0, dst1, dst2, dst3);\n            AVER_UB4_UB(src4, dst4, src5, dst5, src6, dst6, src7, dst7,\n                        dst4, dst5, dst6, dst7);\n            ST_UB8(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst, dst_stride);\n            dst += (8 * dst_stride);\n        }\n    } else if (0 == (height % 4)) {\n        for (cnt = (height / 4); cnt--;) {\n            LD_UB4(src, src_stride, src0, src1, src2, src3);\n            src += (4 * src_stride);\n            LD_UB4(dst, dst_stride, dst0, dst1, dst2, dst3);\n\n            AVER_UB4_UB(src0, dst0, src1, dst1, src2, dst2, src3, dst3,\n                        dst0, dst1, dst2, dst3);\n            ST_UB4(dst0, dst1, dst2, dst3, dst, dst_stride);\n            dst += (4 * dst_stride);\n        }\n    }\n}"}, "post_dep": {}}
{"idx": 13, "category": "non-security", "commit_message": " build: cleanup videotoolbox - there is no need for kCVImageBufferColorPrimaries_ITU_R_2020 checks,&&&&  it's done at runtime&&&&- VideoToolbox presence is now checked with check_apple_framework()&&&&- link to CoreServices is only done when videotoolbox is enabled&&&& ", "diff_code": "diff --git a/configure b/configure\nindex ede5136..3ac0a3c 100755\n--- a/configure\n+++ b/configure\n@@ -1608,7 +1608,6 @@ EXTERNAL_LIBRARY_LIST=\"\n     openal\n     opencl\n     opengl\n-    videotoolbox\n \"\n \n HWACCEL_AUTODETECT_LIBRARY_LIST=\"\n@@ -1622,7 +1621,7 @@ HWACCEL_AUTODETECT_LIBRARY_LIST=\"\n     vaapi\n     vda\n     vdpau\n-    videotoolbox_hwaccel\n+    videotoolbox\n     xvmc\n \"\n \n@@ -2180,7 +2179,6 @@ CONFIG_EXTRA=\"\n     vp3dsp\n     vp56dsp\n     vp8dsp\n-    vt_bt2020\n     wma_freqs\n     wmv2dsp\n \"\n@@ -2938,11 +2936,9 @@ libx265_encoder_deps=\"libx265\"\n libxavs_encoder_deps=\"libxavs\"\n libxvid_encoder_deps=\"libxvid\"\n libzvbi_teletext_decoder_deps=\"libzvbi\"\n-videotoolbox_deps=\"VideoToolbox_VideoToolbox_h\"\n videotoolbox_extralibs=\"-framework CoreFoundation -framework VideoToolbox -framework CoreMedia -framework CoreVideo\"\n videotoolbox_encoder_deps=\"videotoolbox VTCompressionSessionPrepareToEncodeFrames\"\n-videotoolbox_encoder_suggest=\"vda_framework vt_bt2020\"\n-vt_bt2020_deps=\"kCVImageBufferColorPrimaries_ITU_R_2020\"\n+videotoolbox_encoder_suggest=\"vda_framework\"\n \n # demuxers / muxers\n ac3_demuxer_select=\"ac3_parser\"\n@@ -5740,9 +5736,7 @@ check_header termios.h\n check_header unistd.h\n check_header valgrind/valgrind.h\n check_header VideoDecodeAcceleration/VDADecoder.h\n-check_header VideoToolbox/VideoToolbox.h\n check_func_headers VideoToolbox/VTCompressionSession.h VTCompressionSessionPrepareToEncodeFrames -framework VideoToolbox\n-enabled videotoolbox && check_func_headers CoreVideo/CVImageBuffer.h kCVImageBufferColorPrimaries_ITU_R_2020 -framework CoreVideo\n check_header windows.h\n check_header X11/extensions/XvMClib.h\n check_header asm/types.h\n@@ -5757,16 +5751,17 @@ check_lib shell32  \"windows.h shellapi.h\" CommandLineToArgvW   -lshell32\n check_lib wincrypt \"windows.h wincrypt.h\" CryptGenRandom       -ladvapi32\n check_lib psapi    \"windows.h psapi.h\"    GetProcessMemoryInfo -lpsapi\n \n-check_lib coreservices \"CoreServices/CoreServices.h\" UTGetOSTypeFromString \"-framework CoreServices\"\n-\n enabled audiotoolbox && check_apple_framework AudioToolbox\n enabled avfoundation && check_apple_framework AVFoundation\n enabled coreimage    && check_apple_framework CoreImage\n+enabled videotoolbox && check_apple_framework VideoToolbox\n \n enabled avfoundation && {\n     check_lib avfoundation CoreGraphics/CoreGraphics.h               CGGetActiveDisplayList \"-framework CoreGraphics\" ||\n     check_lib avfoundation ApplicationServices/ApplicationServices.h CGGetActiveDisplayList \"-framework ApplicationServices\"; }\n \n+enabled videotoolbox &&\n+    check_lib coreservices CoreServices/CoreServices.h UTGetOSTypeFromString \"-framework CoreServices\"\n \n check_struct \"sys/time.h sys/resource.h\" \"struct rusage\" ru_maxrss\n \n", "ori_dataset": "ffmpeg", "commit_id": "260ea7a7b3", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 14, "category": "non-security", "commit_message": " avformat/concatdec: add fallback for calculating file duration If a file does not have a known duration, this leads to the timestamps&&&&starting over for the next file, causing non-monotonic timestamps.&&&&To prevent this, track the duration during demuxing and use it to&&&&determine the current file duration before opening the next file.&&&&&&&&Signed-off-by: Derek Buitenhuis <derek.buitenhuis@gmail.com>&&&& ", "diff_code": "diff --git a/libavformat/concatdec.c b/libavformat/concatdec.c\nindex e8b37d6..0e18901 100644\n--- a/libavformat/concatdec.c\n+++ b/libavformat/concatdec.c\n@@ -44,6 +44,7 @@ typedef struct {\n     int64_t file_start_time;\n     int64_t file_inpoint;\n     int64_t duration;\n+    int64_t next_dts;\n     ConcatStream *streams;\n     int64_t inpoint;\n     int64_t outpoint;\n@@ -149,6 +150,7 @@ static int add_file(AVFormatContext *avf, char *filename, ConcatFile **rfile,\n     file->url        = url;\n     file->start_time = AV_NOPTS_VALUE;\n     file->duration   = AV_NOPTS_VALUE;\n+    file->next_dts   = AV_NOPTS_VALUE;\n     file->inpoint    = AV_NOPTS_VALUE;\n     file->outpoint   = AV_NOPTS_VALUE;\n \n@@ -509,8 +511,14 @@ static int open_next_file(AVFormatContext *avf)\n     ConcatContext *cat = avf->priv_data;\n     unsigned fileno = cat->cur_file - cat->files;\n \n-    if (cat->cur_file->duration == AV_NOPTS_VALUE)\n-        cat->cur_file->duration = cat->avf->duration - (cat->cur_file->file_inpoint - cat->cur_file->file_start_time);\n+    if (cat->cur_file->duration == AV_NOPTS_VALUE) {\n+        if (cat->avf->duration > 0 || cat->cur_file->next_dts == AV_NOPTS_VALUE) {\n+            cat->cur_file->duration = cat->avf->duration;\n+        } else {\n+            cat->cur_file->duration = cat->cur_file->next_dts;\n+        }\n+        cat->cur_file->duration -= (cat->cur_file->file_inpoint - cat->cur_file->file_start_time);\n+    }\n \n     if (++fileno >= cat->nb_files) {\n         cat->eof = 1;\n@@ -627,6 +635,14 @@ static int concat_read_packet(AVFormatContext *avf, AVPacket *pkt)\n         memcpy(metadata, packed_metadata, metadata_len);\n         av_freep(&packed_metadata);\n     }\n+\n+    if (cat->cur_file->duration == AV_NOPTS_VALUE && st->cur_dts != AV_NOPTS_VALUE) {\n+        int64_t next_dts = av_rescale_q(st->cur_dts, st->time_base, AV_TIME_BASE_Q);\n+        if (cat->cur_file->next_dts == AV_NOPTS_VALUE || next_dts > cat->cur_file->next_dts) {\n+            cat->cur_file->next_dts = next_dts;\n+        }\n+    }\n+\n     return ret;\n }\n \n", "ori_dataset": "ffmpeg", "commit_id": "1a0d9b503d", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 15, "category": "non-security", "commit_message": " d3d11va: Check WINAPI_FAMILY instead of HAVE_LOADLIBRARY If using the winstore compat library, a fallback LoadLibrary&&&&function does exist, that only calls LoadPackagedLibrary though&&&&(which doesn't work for dynamically loading d3d11 DLLs).&&&&&&&&Therefore explicitly check the targeted API family instead.&&&&&&&&Make this check a reusable HAVE_* component which other parts&&&&of the libraries can check when necessary as well.&&&&&&&&Signed-off-by: Martin Storsj <martin@martin.st>&&&&&&&&Merged from Libav commit 4d330da006fe48178.&&&& ", "diff_code": "diff --git a/configure b/configure\nindex 66c7b94..7201941 100755\n--- a/configure\n+++ b/configure\n@@ -2072,6 +2072,7 @@ HAVE_LIST=\"\n     sndio\n     texi2html\n     threads\n+    uwp\n     vaapi_drm\n     vaapi_x11\n     vdpau_x11\n@@ -6134,7 +6135,18 @@ check_func_headers \"windows.h\" CreateDIBSection \"$gdigrab_indev_extralibs\"\n \n # d3d11va requires linking directly to dxgi and d3d11 if not building for\n # the desktop api partition\n-enabled LoadLibrary || d3d11va_extralibs=\"-ldxgi -ld3d11\"\n+check_cpp <<EOF && enable uwp && d3d11va_extralibs=\"-ldxgi -ld3d11\"\n+#ifdef WINAPI_FAMILY\n+#include <winapifamily.h>\n+#if WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP)\n+#error desktop, not uwp\n+#else\n+// WINAPI_FAMILY_APP, WINAPI_FAMILY_PHONE_APP => UWP\n+#endif\n+#else\n+#error no family set\n+#endif\n+EOF\n \n enabled vaapi &&\n     check_lib vaapi va/va.h vaInitialize -lva\ndiff --git a/libavutil/hwcontext_d3d11va.c b/libavutil/hwcontext_d3d11va.c\nindex 9a86d33..52683b9 100644\n--- a/libavutil/hwcontext_d3d11va.c\n+++ b/libavutil/hwcontext_d3d11va.c\n@@ -56,7 +56,7 @@ static PFN_D3D11_CREATE_DEVICE mD3D11CreateDevice;\n \n static av_cold void load_functions(void)\n {\n-#if HAVE_LOADLIBRARY\n+#if !HAVE_UWP\n     // We let these \"leak\" - this is fine, as unloading has no great benefit, and\n     // Windows will mark a DLL as loaded forever if its internal refcount overflows\n     // from too many LoadLibrary calls.\n@@ -486,7 +486,7 @@ static int d3d11va_device_create(AVHWDeviceContext *ctx, const char *device,\n     int ret;\n \n     // (On UWP we can't check this.)\n-#if HAVE_LOADLIBRARY\n+#if !HAVE_UWP\n     if (!LoadLibrary(\"d3d11_1sdklayers.dll\"))\n         is_debug = 0;\n #endif\n@@ -527,7 +527,7 @@ static int d3d11va_device_create(AVHWDeviceContext *ctx, const char *device,\n         ID3D10Multithread_Release(pMultithread);\n     }\n \n-#if HAVE_LOADLIBRARY && HAVE_DXGIDEBUG_H\n+#if !HAVE_UWP && HAVE_DXGIDEBUG_H\n     if (is_debug) {\n         HANDLE dxgidebug_dll = LoadLibrary(\"dxgidebug.dll\");\n         if (dxgidebug_dll) {\n", "ori_dataset": "ffmpeg", "commit_id": "9042402ec7", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 16, "category": "non-security", "commit_message": " avformat/s337m: fix potentially undefined pointer arithmetic Use integer position instead of pointer for loop variable. Also only&&&&skip header fields after header has been fully validated.&&&& ", "diff_code": "diff --git a/libavformat/s337m.c b/libavformat/s337m.c\nindex 1f4ba5e..2e85d48 100644\n--- a/libavformat/s337m.c\n+++ b/libavformat/s337m.c\n@@ -86,22 +86,21 @@ static int s337m_probe(AVProbeData *p)\n {\n     uint64_t state = 0;\n     int markers[3] = { 0 };\n-    int i, sum, max, data_type, data_size, offset;\n+    int i, pos, sum, max, data_type, data_size, offset;\n     uint8_t *buf;\n \n-    for (buf = p->buf; buf < p->buf + p->buf_size; buf++) {\n-        state = (state << 8) | *buf;\n+    for (pos = 0; pos < p->buf_size; pos++) {\n+        state = (state << 8) | p->buf[pos];\n         if (!IS_LE_MARKER(state))\n             continue;\n \n+        buf = p->buf + pos + 1;\n         if (IS_16LE_MARKER(state)) {\n-            data_type = AV_RL16(buf + 1);\n-            data_size = AV_RL16(buf + 3);\n-            buf += 4;\n+            data_type = AV_RL16(buf    );\n+            data_size = AV_RL16(buf + 2);\n         } else {\n-            data_type = AV_RL24(buf + 1);\n-            data_size = AV_RL24(buf + 4);\n-            buf += 6;\n+            data_type = AV_RL24(buf    );\n+            data_size = AV_RL24(buf + 3);\n         }\n \n         if (s337m_get_offset_and_codec(NULL, state, data_type, data_size, &offset, NULL))\n@@ -110,7 +109,8 @@ static int s337m_probe(AVProbeData *p)\n         i = IS_16LE_MARKER(state) ? 0 : IS_20LE_MARKER(state) ? 1 : 2;\n         markers[i]++;\n \n-        buf  += offset;\n+        pos  += IS_16LE_MARKER(state) ? 4 : 6;\n+        pos  += offset;\n         state = 0;\n     }\n \n", "ori_dataset": "ffmpeg", "commit_id": "6029b8a6bb", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 17, "category": "non-security", "commit_message": " vp9: add mxext versions of the single-block (w=4,npx=8) h/v loopfilters. Each takes about 0.5% of runtime in my profiles, and they didn't have&&&&any SIMD yet so far (we only had simd for npx=16 double-block versions).&&&& ", "diff_code": "diff --git a/libavcodec/x86/vp9dsp_init.c b/libavcodec/x86/vp9dsp_init.c\nindex 10751e5..359d38e 100644\n--- a/libavcodec/x86/vp9dsp_init.c\n+++ b/libavcodec/x86/vp9dsp_init.c\n@@ -126,6 +126,7 @@ void ff_vp9_loop_filter_v_##size1##_##size2##_##opt(uint8_t *dst, ptrdiff_t stri\n void ff_vp9_loop_filter_h_##size1##_##size2##_##opt(uint8_t *dst, ptrdiff_t stride, \\\n                                                     int E, int I, int H)\n \n+lpf_funcs(4, 8, mmxext);\n lpf_funcs(16, 16, sse2);\n lpf_funcs(16, 16, ssse3);\n lpf_funcs(16, 16, avx);\n@@ -281,6 +282,8 @@ av_cold void ff_vp9dsp_init_x86(VP9DSPContext *dsp, int bpp, int bitexact)\n     }\n \n     if (EXTERNAL_MMXEXT(cpu_flags)) {\n+        dsp->loop_filter_8[0][0] = ff_vp9_loop_filter_h_4_8_mmxext;\n+        dsp->loop_filter_8[0][1] = ff_vp9_loop_filter_v_4_8_mmxext;\n         init_subpel2(4, 0, 4, put, 8, mmxext);\n         init_subpel2(4, 1, 4, avg, 8, mmxext);\n         init_fpel_func(4, 1,  4, avg, _8, mmxext);\ndiff --git a/libavcodec/x86/vp9lpf.asm b/libavcodec/x86/vp9lpf.asm\nindex 2c4fe21..56bba4d 100644\n--- a/libavcodec/x86/vp9lpf.asm\n+++ b/libavcodec/x86/vp9lpf.asm\n@@ -52,7 +52,7 @@ mask_mix48: times 8 db 0x00\n SECTION .text\n \n %macro SCRATCH 3\n-%if ARCH_X86_64\n+%ifdef m8\n     SWAP                %1, %2\n %else\n     mova              [%3], m%1\n@@ -60,7 +60,7 @@ SECTION .text\n %endmacro\n \n %macro UNSCRATCH 3\n-%if ARCH_X86_64\n+%ifdef m8\n     SWAP                %1, %2\n %else\n     mova               m%1, [%3]\n@@ -69,7 +69,7 @@ SECTION .text\n \n ; %1 = abs(%2-%3)\n %macro ABSSUB 4 ; dst, src1 (RO), src2 (RO), tmp\n-%if ARCH_X86_64\n+%ifdef m8\n     psubusb             %1, %3, %2\n     psubusb             %4, %2, %3\n %else\n@@ -102,7 +102,7 @@ SECTION .text\n %endmacro\n \n %macro UNPACK 4\n-%if ARCH_X86_64\n+%ifdef m8\n     punpck%1bw          %2, %3, %4\n %else\n     mova                %2, %3\n@@ -334,22 +334,24 @@ SECTION .text\n %endmacro\n \n %macro DEFINE_TRANSPOSED_P7_TO_Q7 0-1 0\n-%define P3 rsp +   0 + %1\n-%define P2 rsp +  16 + %1\n-%define P1 rsp +  32 + %1\n-%define P0 rsp +  48 + %1\n-%define Q0 rsp +  64 + %1\n-%define Q1 rsp +  80 + %1\n-%define Q2 rsp +  96 + %1\n-%define Q3 rsp + 112 + %1\n-%define P7 rsp + 128 + %1\n-%define P6 rsp + 144 + %1\n-%define P5 rsp + 160 + %1\n-%define P4 rsp + 176 + %1\n-%define Q4 rsp + 192 + %1\n-%define Q5 rsp + 208 + %1\n-%define Q6 rsp + 224 + %1\n-%define Q7 rsp + 240 + %1\n+%define P3 rsp +  0*mmsize + %1\n+%define P2 rsp +  1*mmsize + %1\n+%define P1 rsp +  2*mmsize + %1\n+%define P0 rsp +  3*mmsize + %1\n+%define Q0 rsp +  4*mmsize + %1\n+%define Q1 rsp +  5*mmsize + %1\n+%define Q2 rsp +  6*mmsize + %1\n+%define Q3 rsp +  7*mmsize + %1\n+%if mmsize == 16\n+%define P7 rsp +  8*mmsize + %1\n+%define P6 rsp +  9*mmsize + %1\n+%define P5 rsp + 10*mmsize + %1\n+%define P4 rsp + 11*mmsize + %1\n+%define Q4 rsp + 12*mmsize + %1\n+%define Q5 rsp + 13*mmsize + %1\n+%define Q6 rsp + 14*mmsize + %1\n+%define Q7 rsp + 15*mmsize + %1\n+%endif\n %endmacro\n \n ; ..............AB -> AAAAAAAABBBBBBBB\n@@ -365,12 +367,12 @@ SECTION .text\n \n %macro LOOPFILTER 5 ; %1=v/h %2=size1 %3+%4=stack, %5=32bit stack only\n %if UNIX64\n-cglobal vp9_loop_filter_%1_%2_16, 5, 9, 16, %3 + %4, dst, stride, E, I, H, mstride, dst2, stride3, mstride3\n+cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 5, 9, 16, %3 + %4, dst, stride, E, I, H, mstride, dst2, stride3, mstride3\n %else\n %if WIN64\n-cglobal vp9_loop_filter_%1_%2_16, 4, 8, 16, %3 + %4, dst, stride, E, I, mstride, dst2, stride3, mstride3\n+cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 4, 8, 16, %3 + %4, dst, stride, E, I, mstride, dst2, stride3, mstride3\n %else\n-cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride, dst2, stride3, mstride3\n+cglobal vp9_loop_filter_%1_%2_ %+ mmsize, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride, dst2, stride3, mstride3\n %define Ed dword r2m\n %define Id dword r3m\n %endif\n@@ -384,18 +386,22 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     lea              mstride3q, [mstrideq*3]\n \n %ifidn %1, h\n-%if %2 > 16\n+%if %2 != 16\n+%if mmsize == 16\n %define movx movh\n+%else\n+%define movx mova\n+%endif\n     lea                   dstq, [dstq + 4*strideq - 4]\n %else\n %define movx movu\n     lea                   dstq, [dstq + 4*strideq - 8] ; go from top center (h pos) to center left (v pos)\n %endif\n-    lea                  dst2q, [dstq + 8*strideq]\n %else\n     lea                   dstq, [dstq + 4*mstrideq]\n-    lea                  dst2q, [dstq + 8*strideq]\n %endif\n+    ; FIXME we shouldn't need two dts registers if mmsize == 8\n+    lea                  dst2q, [dstq + 8*strideq]\n \n     DEFINE_REAL_P7_TO_Q7\n \n@@ -406,11 +412,11 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     movx                    m3, [P4]\n     movx                    m4, [P3]\n     movx                    m5, [P2]\n-%if ARCH_X86_64 || %2 != 16\n+%if (ARCH_X86_64 && mmsize == 16) || %2 > 16\n     movx                    m6, [P1]\n %endif\n     movx                    m7, [P0]\n-%if ARCH_X86_64\n+%ifdef m8\n     movx                    m8, [Q0]\n     movx                    m9, [Q1]\n     movx                   m10, [Q2]\n@@ -502,7 +508,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     movhps        [Q5], m6\n     movhps        [Q7], m7\n     DEFINE_TRANSPOSED_P7_TO_Q7\n-%else ; %2 == 44/48/84/88\n+%elif %2 > 16 ; %2 == 44/48/84/88\n     punpcklbw        m0, m1\n     punpcklbw        m2, m3\n     punpcklbw        m4, m5\n@@ -529,12 +535,31 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     mova           [Q1],  m5\n     mova           [Q2],  m7\n     mova           [Q3],  m3\n+%else ; %2 == 4 || %2 == 8\n+    SBUTTERFLY       bw, 0, 1, 6\n+    SBUTTERFLY       bw, 2, 3, 6\n+    SBUTTERFLY       bw, 4, 5, 6\n+    mova [rsp+4*mmsize], m5\n+    mova             m6, [P1]\n+    SBUTTERFLY       bw, 6, 7, 5\n+    DEFINE_TRANSPOSED_P7_TO_Q7\n+    TRANSPOSE4x4W     0, 2, 4, 6, 5\n+    mova           [P3], m0\n+    mova           [P2], m2\n+    mova           [P1], m4\n+    mova           [P0], m6\n+    mova             m5, [rsp+4*mmsize]\n+    TRANSPOSE4x4W     1, 3, 5, 7, 0\n+    mova           [Q0], m1\n+    mova           [Q1], m3\n+    mova           [Q2], m5\n+    mova           [Q3], m7\n %endif ; %2\n %endif ; x86-32/64\n %endif ; %1 == h\n \n     ; calc fm mask\n-%if %2 == 16\n+%if %2 == 16 || mmsize == 8\n %if cpuflag(ssse3)\n     pxor                m0, m0\n %endif\n@@ -552,7 +577,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     mova                m0, [pb_80]\n     pxor                m2, m0\n     pxor                m3, m0\n-%if ARCH_X86_64\n+%ifdef m8\n %ifidn %1, v\n     mova                m8, [P3]\n     mova                m9, [P2]\n@@ -613,10 +638,10 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n \n     ; (m3: fm, m8..15: p3 p2 p1 p0 q0 q1 q2 q3)\n     ; calc flat8in (if not 44_16) and hev masks\n-%if %2 != 44\n+%if %2 != 44 && %2 != 4\n     mova                m6, [pb_81]                     ; [1 1 1 1 ...] ^ 0x80\n     ABSSUB_GT           m2, rp3, rp0, m6, m5            ; abs(p3 - p0) <= 1\n-%if ARCH_X86_64\n+%ifdef m8\n     mova                m8, [pb_80]\n %define rb80 m8\n %else\n@@ -655,8 +680,15 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n %endif\n %else\n     mova                m6, [pb_80]\n+%if %2 == 44\n     movd                m7, Hd\n     SPLATB_MIX          m7\n+%else\n+%if cpuflag(ssse3)\n+    pxor                m0, m0\n+%endif\n+    SPLATB_REG          m7, H, m0                       ; H H H H ...\n+%endif\n     pxor                m7, m6\n     ABSSUB              m4, rp1, rp0, m1                ; abs(p1 - p0)\n     pxor                m4, m6\n@@ -670,7 +702,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n %if %2 == 16\n     ; (m0: hev, m2: flat8in, m3: fm, m6: pb_81, m9..15: p2 p1 p0 q0 q1 q2 q3)\n     ; calc flat8out mask\n-%if ARCH_X86_64\n+%ifdef m8\n     mova                m8, [P7]\n     mova                m9, [P6]\n %define rp7 m8\n@@ -682,7 +714,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     ABSSUB_GT           m1, rp7, rp0, m6, m5            ; abs(p7 - p0) <= 1\n     ABSSUB_GT           m7, rp6, rp0, m6, m5            ; abs(p6 - p0) <= 1\n     por                 m1, m7\n-%if ARCH_X86_64\n+%ifdef m8\n     mova                m8, [P5]\n     mova                m9, [P4]\n %define rp5 m8\n@@ -695,7 +727,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     por                 m1, m7\n     ABSSUB_GT           m7, rp4, rp0, m6, m5            ; abs(p4 - p0) <= 1\n     por                 m1, m7\n-%if ARCH_X86_64\n+%ifdef m8\n     mova                m14, [Q4]\n     mova                m15, [Q5]\n %define rq4 m14\n@@ -708,7 +740,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     por                 m1, m7\n     ABSSUB_GT           m7, rq5, rq0, m6, m5            ; abs(q5 - q0) <= 1\n     por                 m1, m7\n-%if ARCH_X86_64\n+%ifdef m8\n     mova                m14, [Q6]\n     mova                m15, [Q7]\n %define rq6 m14\n@@ -738,7 +770,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n \n     ; (m0: hev, [m1: flat8out], [m2: flat8in], m3: fm, m8..15: p5 p4 p1 p0 q0 q1 q6 q7)\n     ; filter2()\n-%if %2 != 44\n+%if %2 != 44 && %2 != 4\n     mova                m6, [pb_80]                     ; already in m6 if 44_16\n     SCRATCH              2, 15, rsp+%3+%4\n %if %2 == 16\n@@ -756,7 +788,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     paddsb              m4, m2                          ; 3*(q0 - p0) + (p1 - q1)\n     paddsb              m6, m4, [pb_4]                  ; m6: f1 = clip(f + 4, 127)\n     paddsb              m4, [pb_3]                      ; m4: f2 = clip(f + 3, 127)\n-%if ARCH_X86_64\n+%ifdef m8\n     mova                m14, [pb_10]                    ; will be reused in filter4()\n %define rb10 m14\n %else\n@@ -765,8 +797,8 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     SRSHIFT3B_2X        m6, m4, rb10, m7                ; f1 and f2 sign byte shift by 3\n     SIGN_SUB            m7, rq0, m6, m5                 ; m7 = q0 - f1\n     SIGN_ADD            m1, rp0, m4, m5                 ; m1 = p0 + f2\n-%if %2 != 44\n-%if ARCH_X86_64\n+%if %2 != 44 && %2 != 4\n+%ifdef m8\n     pandn               m6, m15, m3                     ;  ~mask(in) & mask(fm)\n %else\n     mova                m6, [rsp+%3+%4]\n@@ -787,8 +819,8 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     paddsb              m6, m2, [pb_4]                  ; m6:  f1 = clip(f + 4, 127)\n     paddsb              m2, [pb_3]                      ; m2: f2 = clip(f + 3, 127)\n     SRSHIFT3B_2X        m6, m2, rb10, m4                ; f1 and f2 sign byte shift by 3\n-%if %2 != 44\n-%if ARCH_X86_64\n+%if %2 != 44 && %2 != 4\n+%ifdef m8\n     pandn               m5, m15, m3                     ;               ~mask(in) & mask(fm)\n %else\n     mova                m5, [rsp+%3+%4]\n@@ -815,26 +847,26 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     mova                [P1], m1\n     mova                [Q1], m4\n \n-%if %2 != 44\n+%if %2 != 44 && %2 != 4\n     UNSCRATCH            2, 15, rsp+%3+%4\n %endif\n \n     ; ([m1: flat8out], m2: flat8in, m3: fm, m10..13: p1 p0 q0 q1)\n     ; filter6()\n-%if %2 != 44\n+%if %2 != 44 && %2 != 4\n     pxor                m0, m0\n %if %2 > 16\n     pand                m3, m2\n %else\n     pand                m2, m3                          ;               mask(fm) & mask(in)\n-%if ARCH_X86_64\n+%ifdef m8\n     pandn               m3, m8, m2                      ; ~mask(out) & (mask(fm) & mask(in))\n %else\n     mova                m3, [rsp+%3+%4+16]\n     pandn               m3, m2\n %endif\n %endif\n-%if ARCH_X86_64\n+%ifdef m8\n     mova               m14, [P3]\n     mova                m9, [Q3]\n %define rp3 m14\n@@ -882,7 +914,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     pand            m1, m2                                                              ; mask(out) & (mask(fm) & mask(in))\n     mova            m2, [P7]\n     mova            m3, [P6]\n-%if ARCH_X86_64\n+%ifdef m8\n     mova            m8, [P5]\n     mova            m9, [P4]\n %define rp5 m8\n@@ -1008,7 +1040,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     movhps [Q5],  m6\n     movhps [Q7],  m7\n %endif\n-%elif %2 == 44\n+%elif %2 == 44 || %2 == 4\n     SWAP 0, 1   ; m0 = p1\n     SWAP 1, 7   ; m1 = p0\n     SWAP 2, 5   ; m2 = q0\n@@ -1018,6 +1050,7 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     SBUTTERFLY  bw, 2, 3, 4\n     SBUTTERFLY  wd, 0, 2, 4\n     SBUTTERFLY  wd, 1, 3, 4\n+%if mmsize == 16\n     movd  [P7], m0\n     movd  [P3], m2\n     movd  [Q0], m1\n@@ -1047,6 +1080,20 @@ cglobal vp9_loop_filter_%1_%2_16, 2, 6, 16, %3 + %4 + %5, dst, stride, mstride,\n     movd  [Q3], m1\n     movd  [Q7], m3\n %else\n+    movd  [P7], m0\n+    movd  [P5], m2\n+    movd  [P3], m1\n+    movd  [P1], m3\n+    psrlq   m0, 32\n+    psrlq   m2, 32\n+    psrlq   m1, 32\n+    psrlq   m3, 32\n+    movd  [P6], m0\n+    movd  [P4], m2\n+    movd  [P2], m1\n+    movd  [P0], m3\n+%endif\n+%else\n     ; the following code do a transpose of 8 full lines to 16 half\n     ; lines (high part). It is inlined to avoid the need of a staging area\n     mova                    m0, [P3]\n@@ -1137,3 +1184,7 @@ LPF_16_VH_ALL_OPTS 44,   0, 128,  0\n LPF_16_VH_ALL_OPTS 48, 256, 128, 16\n LPF_16_VH_ALL_OPTS 84, 256, 128, 16\n LPF_16_VH_ALL_OPTS 88, 256, 128, 16\n+\n+INIT_MMX mmxext\n+LOOPFILTER v, 4, 0,  0, 0\n+LOOPFILTER h, 4, 0, 64, 0\n", "ori_dataset": "ffmpeg", "commit_id": "7ca422bb1b", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 18, "category": "non-security", "commit_message": " checkasm: add AAC PS tests This includes various fixes and improvements from James Almer.&&&&&&&&Signed-off-by: James Almer <jamrial@gmail.com>&&&& ", "diff_code": "diff --git a/tests/checkasm/Makefile b/tests/checkasm/Makefile\nindex 73c2b35..638e811 100644\n--- a/tests/checkasm/Makefile\n+++ b/tests/checkasm/Makefile\n@@ -13,6 +13,7 @@ AVCODECOBJS-$(CONFIG_VP8DSP)            += vp8dsp.o\n AVCODECOBJS-$(CONFIG_VIDEODSP)          += videodsp.o\n \n # decoders/encoders\n+AVCODECOBJS-$(CONFIG_AAC_DECODER)       += aacpsdsp.o\n AVCODECOBJS-$(CONFIG_ALAC_DECODER)      += alacdsp.o\n AVCODECOBJS-$(CONFIG_DCA_DECODER)       += synth_filter.o\n AVCODECOBJS-$(CONFIG_JPEG2000_DECODER)  += jpeg2000dsp.o\ndiff --git a/tests/checkasm/aacpsdsp.c b/tests/checkasm/aacpsdsp.c\nnew file mode 100644\nindex 0000000..a0dfa16\n--- /dev/null\n+++ b/tests/checkasm/aacpsdsp.c\n@@ -0,0 +1,161 @@\n+/*\n+ * This file is part of FFmpeg.\n+ *\n+ * FFmpeg is free software; you can redistribute it and/or modify\n+ * it under the terms of the GNU General Public License as published by\n+ * the Free Software Foundation; either version 2 of the License, or\n+ * (at your option) any later version.\n+ *\n+ * FFmpeg is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU General Public License along\n+ * with FFmpeg; if not, write to the Free Software Foundation, Inc.,\n+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n+ */\n+\n+#include \"libavcodec/aacpsdsp.h\"\n+\n+#include \"checkasm.h\"\n+\n+#define N 32\n+#define STRIDE 128\n+#define BUF_SIZE (N * STRIDE)\n+\n+#define randomize(buf, len) do {                                \\\n+    int i;                                                      \\\n+    for (i = 0; i < len; i++) {                                 \\\n+        const INTFLOAT f = (INTFLOAT)rnd() / UINT_MAX;          \\\n+        (buf)[i] = f;                                           \\\n+    }                                                           \\\n+} while (0)\n+\n+#define EPS 0.005\n+\n+static void test_add_squares(void)\n+{\n+    LOCAL_ALIGNED_16(INTFLOAT, dst0, [BUF_SIZE]);\n+    LOCAL_ALIGNED_16(INTFLOAT, dst1, [BUF_SIZE]);\n+    LOCAL_ALIGNED_16(INTFLOAT, src, [BUF_SIZE], [2]);\n+\n+    declare_func(void, INTFLOAT *dst,\n+                 const INTFLOAT (*src)[2], int n);\n+\n+    randomize((INTFLOAT *)src, BUF_SIZE * 2);\n+    randomize(dst0, BUF_SIZE);\n+    memcpy(dst1, dst0, BUF_SIZE * sizeof(INTFLOAT));\n+    call_ref(dst0, src, BUF_SIZE);\n+    call_new(dst1, src, BUF_SIZE);\n+    if (!float_near_abs_eps_array(dst0, dst1, EPS, BUF_SIZE))\n+        fail();\n+    bench_new(dst1, src, BUF_SIZE);\n+}\n+\n+static void test_mul_pair_single(void)\n+{\n+    LOCAL_ALIGNED_16(INTFLOAT, dst0, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, dst1, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, src0, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, src1, [BUF_SIZE]);\n+\n+    declare_func(void, INTFLOAT (*dst)[2],\n+                       INTFLOAT (*src0)[2], INTFLOAT *src1, int n);\n+\n+    randomize((INTFLOAT *)src0, BUF_SIZE * 2);\n+    randomize(src1, BUF_SIZE);\n+    call_ref(dst0, src0, src1, BUF_SIZE);\n+    call_new(dst1, src0, src1, BUF_SIZE);\n+    if (!float_near_abs_eps_array((float *)dst0, (float *)dst1, EPS, BUF_SIZE * 2))\n+        fail();\n+    bench_new(dst1, src0, src1, BUF_SIZE);\n+}\n+\n+static void test_hybrid_analysis(void)\n+{\n+    LOCAL_ALIGNED_16(INTFLOAT, dst0, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, dst1, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, in, [12], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, filter, [N], [8][2]);\n+\n+    declare_func(void, INTFLOAT (*out)[2], INTFLOAT (*in)[2],\n+                 const INTFLOAT (*filter)[8][2],\n+                 int stride, int n);\n+\n+    randomize((INTFLOAT *)in, 12 * 2);\n+    randomize((INTFLOAT *)filter, N * 8 * 2);\n+\n+    randomize((INTFLOAT *)dst0, BUF_SIZE * 2);\n+    memcpy(dst1, dst0, BUF_SIZE * 2 * sizeof(INTFLOAT));\n+\n+    call_ref(dst0, in, filter, STRIDE, N);\n+    call_new(dst1, in, filter, STRIDE, N);\n+\n+    if (!float_near_abs_eps_array((float *)dst0, (float *)dst1, EPS, BUF_SIZE * 2))\n+        fail();\n+    bench_new(dst1, in, filter, STRIDE, N);\n+}\n+\n+static void test_stereo_interpolate(PSDSPContext *psdsp)\n+{\n+    int i;\n+    LOCAL_ALIGNED_16(INTFLOAT, l,  [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, r,  [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, l0, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, r0, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, l1, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, r1, [BUF_SIZE], [2]);\n+    LOCAL_ALIGNED_16(INTFLOAT, h, [2], [4]);\n+    LOCAL_ALIGNED_16(INTFLOAT, h_step, [2], [4]);\n+\n+    declare_func(void, INTFLOAT (*l)[2], INTFLOAT (*r)[2],\n+                       INTFLOAT h[2][4], INTFLOAT h_step[2][4], int len);\n+\n+    randomize((INTFLOAT *)l, BUF_SIZE * 2);\n+    randomize((INTFLOAT *)r, BUF_SIZE * 2);\n+\n+    for (i = 0; i < 2; i++) {\n+        if (check_func(psdsp->stereo_interpolate[i], \"ps_stereo_interpolate%s\", i ? \"_ipdopd\" : \"\")) {\n+            memcpy(l0, l, BUF_SIZE * 2 * sizeof(INTFLOAT));\n+            memcpy(l1, l, BUF_SIZE * 2 * sizeof(INTFLOAT));\n+            memcpy(r0, r, BUF_SIZE * 2 * sizeof(INTFLOAT));\n+            memcpy(r1, r, BUF_SIZE * 2 * sizeof(INTFLOAT));\n+\n+            randomize((INTFLOAT *)h, 2 * 4);\n+            randomize((INTFLOAT *)h_step, 2 * 4);\n+\n+            call_ref(l0, r0, h, h_step, BUF_SIZE);\n+            call_new(l1, r1, h, h_step, BUF_SIZE);\n+            if (!float_near_abs_eps_array((float *)l0, (float *)l1, EPS, BUF_SIZE * 2) ||\n+                !float_near_abs_eps_array((float *)r0, (float *)r1, EPS, BUF_SIZE * 2))\n+                fail();\n+\n+            memcpy(l1, l, BUF_SIZE * 2 * sizeof(INTFLOAT));\n+            memcpy(r1, r, BUF_SIZE * 2 * sizeof(INTFLOAT));\n+            bench_new(l1, r1, h, h_step, BUF_SIZE);\n+        }\n+    }\n+}\n+\n+void checkasm_check_aacpsdsp(void)\n+{\n+    PSDSPContext psdsp;\n+\n+    ff_psdsp_init(&psdsp);\n+\n+    if (check_func(psdsp.add_squares, \"ps_add_squares\"))\n+        test_add_squares();\n+    report(\"add_squares\");\n+\n+    if (check_func(psdsp.mul_pair_single, \"ps_mul_pair_single\"))\n+        test_mul_pair_single();\n+    report(\"mul_pair_single\");\n+\n+    if (check_func(psdsp.hybrid_analysis, \"ps_hybrid_analysis\"))\n+        test_hybrid_analysis();\n+    report(\"hybrid_analysis\");\n+\n+    test_stereo_interpolate(&psdsp);\n+    report(\"stereo_interpolate\");\n+}\ndiff --git a/tests/checkasm/checkasm.c b/tests/checkasm/checkasm.c\nindex 0b86627..e66744b 100644\n--- a/tests/checkasm/checkasm.c\n+++ b/tests/checkasm/checkasm.c\n@@ -65,6 +65,9 @@ static const struct {\n     void (*func)(void);\n } tests[] = {\n #if CONFIG_AVCODEC\n+    #if CONFIG_AAC_DECODER\n+        { \"aacpsdsp\", checkasm_check_aacpsdsp },\n+    #endif\n     #if CONFIG_ALAC_DECODER\n         { \"alacdsp\", checkasm_check_alacdsp },\n     #endif\ndiff --git a/tests/checkasm/checkasm.h b/tests/checkasm/checkasm.h\nindex 2093210..dfb0ce5 100644\n--- a/tests/checkasm/checkasm.h\n+++ b/tests/checkasm/checkasm.h\n@@ -31,6 +31,7 @@\n #include \"libavutil/lfg.h\"\n #include \"libavutil/timer.h\"\n \n+void checkasm_check_aacpsdsp(void);\n void checkasm_check_alacdsp(void);\n void checkasm_check_audiodsp(void);\n void checkasm_check_blend(void);\ndiff --git a/tests/fate/checkasm.mak b/tests/fate/checkasm.mak\nindex cf84f47..61ce6cd 100644\n--- a/tests/fate/checkasm.mak\n+++ b/tests/fate/checkasm.mak\n@@ -1,4 +1,5 @@\n-FATE_CHECKASM = fate-checkasm-alacdsp                                   \\\n+FATE_CHECKASM = fate-checkasm-aacpsdsp                                  \\\n+                fate-checkasm-alacdsp                                   \\\n                 fate-checkasm-audiodsp                                  \\\n                 fate-checkasm-blockdsp                                  \\\n                 fate-checkasm-bswapdsp                                  \\\n", "ori_dataset": "ffmpeg", "commit_id": "edd041e64c", "dependency": {"float_near_abs_eps_array": "int float_near_abs_eps_array(const float *a, const float *b, float eps,\n                         unsigned len)\n{\n    unsigned i;\n\n    for (i = 0; i < len; i++) {\n        if (!float_near_abs_eps(a[i], b[i], eps))\n            return 0;\n    }\n    return 1;\n}", "randomize": "static int randomize(uint8_t *buf, int len)\n{\n#if CONFIG_GCRYPT\n    gcry_randomize(buf, len, GCRY_VERY_STRONG_RANDOM);\n    return 0;\n#elif CONFIG_OPENSSL\n    if (RAND_bytes(buf, len))\n        return 0;\n#else\n    return AVERROR(ENOSYS);\n#endif\n    return AVERROR(EINVAL);\n}", "check_func": "static void check_func(int value, int line, const char *msg, ...)\n{\n    if (!value) {\n        va_list ap;\n        va_start(ap, msg);\n        printf(\"%d: \", line);\n        vprintf(msg, ap);\n        printf(\"\\n\");\n        check_faults++;\n        va_end(ap);\n    }\n}", "ff_psdsp_init": "", "INTFLOAT": "", "bench_new": "", "declare_func": "", "call_new": "", "report": "", "memcpy": "", "call_ref": "", "fail": "", "LOCAL_ALIGNED_16": ""}, "pre_dep": {}, "post_dep": {"float_near_abs_eps_array": "int float_near_abs_eps_array(const float *a, const float *b, float eps,\n                         unsigned len)\n{\n    unsigned i;\n\n    for (i = 0; i < len; i++) {\n        if (!float_near_abs_eps(a[i], b[i], eps))\n            return 0;\n    }\n    return 1;\n}", "randomize": "static int randomize(uint8_t *buf, int len)\n{\n#if CONFIG_GCRYPT\n    gcry_randomize(buf, len, GCRY_VERY_STRONG_RANDOM);\n    return 0;\n#elif CONFIG_OPENSSL\n    if (RAND_bytes(buf, len))\n        return 0;\n#else\n    return AVERROR(ENOSYS);\n#endif\n    return AVERROR(EINVAL);\n}", "check_func": "static void check_func(int value, int line, const char *msg, ...)\n{\n    if (!value) {\n        va_list ap;\n        va_start(ap, msg);\n        printf(\"%d: \", line);\n        vprintf(msg, ap);\n        printf(\"\\n\");\n        check_faults++;\n        va_end(ap);\n    }\n}"}}
{"idx": 19, "category": "non-security", "commit_message": " avformat/hlsenc: copy codec_tag when stream copy when use fmp4 segment type in hls and use codec copy,&&&&there have an error message.&&&&error message:&&&&   [mp4 @ 0x25df020] Tag avc1 incompatible with output codec id '28' ([33][0][0][0])&&&&   [hls @ 0x2615c80] Some of the provided format options in '(null)' are not recognized&&&&   Could not write header for output file #0 (incorrect codec parameters ?): Invalid argument&&&&this patch can fix it.&&&&&&&&Signed-off-by: Liu Qi <w_liuqi@kingsoft.com>&&&&Signed-off-by: Steven Liu <lq@onvideo.cn>&&&& ", "diff_code": "diff --git a/libavformat/hlsenc.c b/libavformat/hlsenc.c\nindex 2fe07f0..dd6a62b 100644\n--- a/libavformat/hlsenc.c\n+++ b/libavformat/hlsenc.c\n@@ -566,6 +566,14 @@ static int hls_mux_init(AVFormatContext *s)\n         if (!(st = avformat_new_stream(loc, NULL)))\n             return AVERROR(ENOMEM);\n         avcodec_parameters_copy(st->codecpar, s->streams[i]->codecpar);\n+        if (!oc->oformat->codec_tag ||\n+            av_codec_get_id (oc->oformat->codec_tag, s->streams[i]->codecpar->codec_tag) == st->codecpar->codec_id ||\n+            av_codec_get_tag(oc->oformat->codec_tag, s->streams[i]->codecpar->codec_id) <= 0) {\n+            st->codecpar->codec_tag = s->streams[i]->codecpar->codec_tag;\n+        } else {\n+            st->codecpar->codec_tag = 0;\n+        }\n+\n         st->sample_aspect_ratio = s->streams[i]->sample_aspect_ratio;\n         st->time_base = s->streams[i]->time_base;\n         av_dict_copy(&st->metadata, s->streams[i]->metadata, 0);\n", "ori_dataset": "ffmpeg", "commit_id": "1fe40e73a6", "dependency": {"av_codec_get_tag": "unsigned int av_codec_get_tag(const AVCodecTag *const *tags, enum AVCodecID id)\n{\n    unsigned int tag;\n    if (!av_codec_get_tag2(tags, id, &tag))\n        return 0;\n    return tag;\n}", "av_codec_get_id": "enum AVCodecID av_codec_get_id(const AVCodecTag *const *tags, unsigned int tag)\n{\n    int i;\n    for (i = 0; tags && tags[i]; i++) {\n        enum AVCodecID id = ff_codec_get_id(tags[i], tag);\n        if (id != AV_CODEC_ID_NONE)\n            return id;\n    }\n    return AV_CODEC_ID_NONE;\n}"}, "pre_dep": {}, "post_dep": {"av_codec_get_tag": "unsigned int av_codec_get_tag(const AVCodecTag *const *tags, enum AVCodecID id)\n{\n    unsigned int tag;\n    if (!av_codec_get_tag2(tags, id, &tag))\n        return 0;\n    return tag;\n}", "av_codec_get_id": "enum AVCodecID av_codec_get_id(const AVCodecTag *const *tags, unsigned int tag)\n{\n    int i;\n    for (i = 0; tags && tags[i]; i++) {\n        enum AVCodecID id = ff_codec_get_id(tags[i], tag);\n        if (id != AV_CODEC_ID_NONE)\n            return id;\n    }\n    return AV_CODEC_ID_NONE;\n}"}}
{"idx": 20, "category": "non-security", "commit_message": " checkasm: add missing checks to float_dsp's butterflies_float test  ", "diff_code": "diff --git a/tests/checkasm/float_dsp.c b/tests/checkasm/float_dsp.c\nindex 641cc6f..9b0a221 100644\n--- a/tests/checkasm/float_dsp.c\n+++ b/tests/checkasm/float_dsp.c\n@@ -219,9 +219,12 @@ static void test_butterflies_float(const float *src0, const float *src1)\n     call_ref(cdst, cdst1, LEN);\n     call_new(odst, odst1, LEN);\n     for (i = 0; i < LEN; i++) {\n-        if (!float_near_abs_eps(cdst[i], odst[i], FLT_EPSILON)) {\n+        if (!float_near_abs_eps(cdst[i],  odst[i],  FLT_EPSILON) ||\n+            !float_near_abs_eps(cdst1[i], odst1[i], FLT_EPSILON)) {\n             fprintf(stderr, \"%d: %- .12f - %- .12f = % .12g\\n\",\n                     i, cdst[i], odst[i], cdst[i] - odst[i]);\n+            fprintf(stderr, \"%d: %- .12f - %- .12f = % .12g\\n\",\n+                    i, cdst1[i], odst1[i], cdst1[i] - odst1[i]);\n             fail();\n             break;\n         }\n", "ori_dataset": "ffmpeg", "commit_id": "a579dbb4f7", "dependency": {}, "pre_dep": {}, "post_dep": {}}
{"idx": 5, "category": "non-security", "commit_message": " libavcodec/h264_sei: Don't log random user data. This prevents terminal junk.  ", "diff_code": "diff --git a/libavcodec/h264_sei.c b/libavcodec/h264_sei.c\nindex 332ae50..ae5f39f 100644\n--- a/libavcodec/h264_sei.c\n+++ b/libavcodec/h264_sei.c\n@@ -257,9 +257,6 @@ static int decode_unregistered_user_data(H264SEIUnregistered *h, GetBitContext *\n     if (e == 1 && build == 1 && !strncmp(user_data+16, \"x264 - core 0000\", 16))\n         h->x264_build = 67;\n \n-    if (strlen(user_data + 16) > 0)\n-        av_log(logctx, AV_LOG_DEBUG, \"user data:\\\"%s\\\"\\n\", user_data + 16);\n-\n     av_free(user_data);\n     return 0;\n }\n", "ori_dataset": "ffmpeg", "commit_id": "1f28a991ef", "dependency": {"av_log": "void av_log(void* avcl, int level, const char *fmt, ...)\n{\n    AVClass* avc = avcl ? *(AVClass **) avcl : NULL;\n    va_list vl;\n    va_start(vl, fmt);\n    if (avc && avc->version >= (50 << 16 | 15 << 8 | 2) &&\n        avc->log_level_offset_offset && level >= AV_LOG_FATAL)\n        level += *(int *) (((uint8_t *) avcl) + avc->log_level_offset_offset);\n    av_vlog(avcl, level, fmt, vl);\n    va_end(vl);\n}", "strlen": ""}, "pre_dep": {"av_log": "void av_log(void* avcl, int level, const char *fmt, ...)\n{\n    AVClass* avc = avcl ? *(AVClass **) avcl : NULL;\n    va_list vl;\n    va_start(vl, fmt);\n    if (avc && avc->version >= (50 << 16 | 15 << 8 | 2) &&\n        avc->log_level_offset_offset && level >= AV_LOG_FATAL)\n        level += *(int *) (((uint8_t *) avcl) + avc->log_level_offset_offset);\n    av_vlog(avcl, level, fmt, vl);\n    va_end(vl);\n}"}, "post_dep": {}}
